{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "word-embeddings-elmo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vondersam/sdgs_text_classifier/blob/master/experiments/word_embeddings_elmo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBt_T5zUMG68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install -I keras\n",
        "#!pip install iterative-stratification\n",
        "#!pip install -U sacremoses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "yzkDHu1qhHIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, hamming_loss\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.initializers import Constant\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "from keras import backend as K\n",
        "import keras.layers as layers\n",
        "from keras.models import Model, load_model\n",
        "from keras.engine import Layer\n",
        "\n",
        "from sacremoses import MosesTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ld3ptbEqsA",
        "colab_type": "code",
        "outputId": "1a8a5597-ba6e-4682-9e85-ee56a0d9b3d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "base_dir = \"gdrive/My Drive/fastai-v3/sdgs/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi63MqzME0Ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT_DATA_DIR = f\"{base_dir}dataset/cleanup_labelled.csv\"\n",
        "ELMO_DIR = f\"{base_dir}embeddings/elmo/\"\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 50\n",
        "VALIDATION_SPLIT = 0.2\n",
        "labels_index = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_nighR5UdrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ElmoEmbeddingLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.dimensions = 1024\n",
        "        self.trainable=True\n",
        "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n",
        "                               name=\"{}_module\".format(self.name))\n",
        "        \n",
        "\n",
        "        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
        "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
        "                      as_dict=True,\n",
        "                      signature='default',\n",
        "                      )['default']\n",
        "        return result\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return K.not_equal(inputs, '--PAD--')\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.dimensions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJkKGmTooe0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to build model\n",
        "def build_model(): \n",
        "  input_text = layers.Input(shape=(1,), dtype=\"string\")\n",
        "  embedding = ElmoEmbeddingLayer()(input_text)\n",
        "  dense = layers.Dense(256, activation='relu')(embedding)\n",
        "  pred = layers.Dense(17, activation='sigmoid')(dense)\n",
        "\n",
        "  model = Model(inputs=[input_text], outputs=pred)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c4ZtiXcoy23",
        "colab_type": "code",
        "outputId": "1f669ce9-673e-49c6-e782-58588a068a5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('Processing text dataset')\n",
        "df = pd.read_csv(TEXT_DATA_DIR)\n",
        "df.labels = df.labels.str.split('|').apply(lambda x: [int(i) for i in x])\n",
        "\n",
        "train_df, test_df = df.iloc[0:3000], df.iloc[3001:]\n",
        "\n",
        "train_text = train_df['text'].tolist()\n",
        "train_text = [' '.join(t.split()[0:150]) for t in train_text]\n",
        "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "train_label = mlb.fit_transform(train_df.labels)\n",
        "\n",
        "test_text = test_df['text'].tolist()\n",
        "test_text = [' '.join(t.split()[0:150]) for t in test_text]\n",
        "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "test_label = mlb.fit_transform(test_df.labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing text dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RprlzS9qFXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()\n",
        "model.fit(train_text, \n",
        "          train_label,\n",
        "          epochs=1,\n",
        "          batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th1sQhHhMl0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(TEXT_DATA_DIR)\n",
        "df.labels = df.labels.str.split('|').apply(lambda x: [int(i) for i in x])\n",
        "\n",
        "train_df, test_df = df.iloc[0:3000], df.iloc[3001:]\n",
        "\n",
        "train_text = train_df['text'].tolist()\n",
        "train_text = [' '.join(t.split()[0:150]) for t in train_text]\n",
        "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "train_label = mlb.fit_transform(train_df.labels)\n",
        "\n",
        "test_text = test_df['text'].tolist()\n",
        "test_text = [' '.join(t.split()[0:150]) for t in test_text]\n",
        "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "test_label = mlb.fit_transform(test_df.labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYvfb0fCOX_a",
        "colab_type": "code",
        "outputId": "f8da6174-991b-49eb-a620-a2748267890a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# second, prepare text samples and their labels\n",
        "print('Processing text dataset')\n",
        "df = pd.read_csv(TEXT_DATA_DIR)\n",
        "df.labels = df.labels.str.split('|').apply(lambda x: [int(i) for i in x])\n",
        "\n",
        "print('Tokenizing data')\n",
        "data = np.array([mt.tokenize(t, escape=False)[:150] for t in df.text])\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels = mlb.fit_transform(df.labels)\n",
        "\n",
        "# Cross-validation: split the data into a training set and a test set\n",
        "\n",
        "count = 0\n",
        "mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0)\n",
        "\n",
        "for train_index, test_index in mskf.split(data, labels):\n",
        "    count += 1\n",
        "    print(f\"Fold no. {count}\")\n",
        "    train_text, test_text = data[train_index], data[test_index]\n",
        "    \n",
        "    # Look into adapting the script to accept list of tokens instead\n",
        "    train_text = [' '.join(t) for t in train_text]\n",
        "    train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "    \n",
        "    test_text = [' '.join(t) for t in test_text]\n",
        "    test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "    \n",
        "    train_label, test_label = labels[train_index], labels[test_index]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing text dataset\n",
            "Tokenizing data\n",
            "Fold no. 1\n",
            "Fold no. 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHtvFIu0Zsyx",
        "colab_type": "code",
        "outputId": "c7fc1dc7-4ed3-42fe-bfa4-a8a911bfc317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "model = build_model()\n",
        "model.fit(train_text, \n",
        "          train_label,\n",
        "          epochs=2,\n",
        "          batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "elmo_embedding_layer_8 (Elmo (None, 1024)              4         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 17)                4369      \n",
            "=================================================================\n",
            "Total params: 266,773\n",
            "Trainable params: 266,773\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "2577/2577 [==============================] - 4398s 2s/step - loss: 0.3065 - acc: 0.9092\n",
            "Epoch 2/2\n",
            "2577/2577 [==============================] - 4492s 2s/step - loss: 0.2429 - acc: 0.9226\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fba0c5bb1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7DNRwlTFv_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_avg(models_testx_testy, labels_, thres=0.3):\n",
        "  def calc(model, test_x, test_y):\n",
        "    predictions = model.predict(test_x)>thres\n",
        "    metrics = classification_report(test_y, predictions, target_names=labels_, output_dict=True)\n",
        "    metrics_df = pd.DataFrame.from_dict(metrics)\n",
        "    h = hamming_loss(test_y, predictions)\n",
        "    roc = roc_auc_score(test_y, predictions, average='micro')\n",
        "    return metrics_df, h, roc\n",
        "    \n",
        "  model_1, test_x_1, test_y_1 = models_testx_testy[0]\n",
        "  metrics_agg, ham, roc = calc(model_1, test_x_1, test_y_1)\n",
        "  n = len(models_testx_testy)\n",
        "  \n",
        "  for model, test_x, test_y_1 in models_testx_testy[1:]:\n",
        "    metrics, h, r = calc(model, test_x, test_y_1)\n",
        "    metrics_agg += metrics\n",
        "    ham += h\n",
        "    roc += r\n",
        "  \n",
        "  return metrics_agg/n, ham/n, roc/n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFTTFXJ_F3nG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(ELMO_DIR + f\"2_epoch-elmo.h5\")   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT92tW4qGCsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [str(i) for i in range(1,18)]\n",
        "averaged_results = metrics_avg([(model, test_text, test_label)], labels)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}