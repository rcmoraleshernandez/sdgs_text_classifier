{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "word-embeddings-glove.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vondersam/sdgs_text_classifier/blob/master/experiments/word_embeddings_glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBt_T5zUMG68",
        "colab_type": "code",
        "outputId": "f9210a29-88b0-4846-e502-c94dbf031611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "#!pip install -I tensorflow\n",
        "#!pip install -I keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting iterative-stratification\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.17.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.13.2)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "yzkDHu1qhHIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score, hamming_loss, accuracy_score\n",
        "from keras import optimizers\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "import os\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Model\n",
        "from keras.initializers import Constant"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ld3ptbEqsA",
        "colab_type": "code",
        "outputId": "c38ca285-004b-493e-938a-4a7c0d3be8df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "base_dir = \"gdrive/My Drive/fastai-v3/sdgs/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi63MqzME0Ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT_DATA_DIR = f\"{base_dir}dataset/cleanup_labelled.csv\"\n",
        "GLOVE_DIR = f\"{base_dir}embeddings/glove/glove.6B/\"\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 300\n",
        "labels_index = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-DVSj0NhHIj",
        "colab_type": "code",
        "outputId": "9fa2c556-facf-4248-a9ff-ed9906523061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "# Load pretrained embeddings in an index mapping words in the embeddings set\n",
        "# to their embeddings vector\n",
        "print('Indexing word vectors.')\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt')) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")\n",
        "\n",
        "# second, prepare text samples and their labels\n",
        "print('Processing text dataset')\n",
        "df = pd.read_csv(TEXT_DATA_DIR)\n",
        "df.labels = df.labels.str.split('|').apply(lambda x: [int(i) for i in x])\n",
        "\n",
        "# finally, vectorize the text samples into a 2D integer tensor\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(df.text)\n",
        "sequences = tokenizer.texts_to_sequences(df.text)\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels = mlb.fit_transform(df.labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# Cross-validation: split the data into a training set and a test set\n",
        "count = 0\n",
        "mskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n",
        "cross_entropy_files = []\n",
        "\n",
        "for train_index, test_index in mskf.split(data, labels):\n",
        "    count += 1\n",
        "    print(f\"Fold no. {count}\")\n",
        "    x_train, x_val = data[train_index], data[test_index]\n",
        "    y_train, y_val = labels[train_index], labels[test_index]\n",
        "    cross_entropy_files.append((x_train, x_val, y_train, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n",
            "Processing text dataset\n",
            "Found 14736 unique tokens.\n",
            "Shape of data tensor: (5182, 1000)\n",
            "Shape of label tensor: (5182, 17)\n",
            "Fold no. 1\n",
            "Fold no. 2\n",
            "Fold no. 3\n",
            "Fold no. 4\n",
            "Fold no. 5\n",
            "Fold no. 6\n",
            "Fold no. 7\n",
            "Fold no. 8\n",
            "Fold no. 9\n",
            "Fold no. 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLEjjQVeSvVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "results = []\n",
        "for x_train, x_val, y_train, y_val in cross_entropy_files:\n",
        "    count += 1\n",
        "    print(F\"Training fold {count}\")\n",
        "    print('Preparing embedding matrix.')\n",
        "    # prepare embedding matrix\n",
        "    num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        if i > MAX_NUM_WORDS:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    # load pre-trained word embeddings into an Embedding layer\n",
        "    # note that we set trainable = False so as to keep the embeddings fixed\n",
        "    embedding_layer = Embedding(num_words,\n",
        "                                EMBEDDING_DIM,\n",
        "                                embeddings_initializer=Constant(embedding_matrix),\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=False)\n",
        "\n",
        "    print('Training model.')\n",
        "    # train a 1D convnet with global maxpooling\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "    x = MaxPooling1D(5)(x)\n",
        "    x = Conv1D(128, 5, activation='relu')(x)\n",
        "    x = MaxPooling1D(5)(x)\n",
        "    x = Conv1D(128, 5, activation='relu')(x)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    preds = Dense(len(labels_index), activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='binary_crossentropy', \n",
        "                optimizer='Adam', \n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    model.fit(x_train, y_train,\n",
        "            batch_size=128,\n",
        "            epochs=10,\n",
        "            validation_data=(x_val, y_val))\n",
        "    results.append((model, x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTa2le7E1MM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "for model, t_x, t_y in results:\n",
        "    count += 1\n",
        "    model.save(GLOVE_DIR + f\"10_epoch-300d-10-fold-cross-val_{count}.h5\")   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip3GMV8X03J_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_avg(models_testx_testy, labels_, thres=0.3):\n",
        "  def calc(model, test_x, test_y):\n",
        "    predictions = model.predict(test_x)>thres\n",
        "    metrics = classification_report(test_y, predictions, target_names=labels_, output_dict=True)\n",
        "    metrics_df = pd.DataFrame.from_dict(metrics)\n",
        "    h = hamming_loss(test_y, predictions)\n",
        "    roc = roc_auc_score(test_y, predictions, average='micro')\n",
        "    return metrics_df, h, roc\n",
        "    \n",
        "  model_1, test_x_1, test_y_1 = models_testx_testy[0]\n",
        "  metrics_agg, ham, roc = calc(model_1, test_x_1, test_y_1)\n",
        "  n = len(models_testx_testy)\n",
        "  \n",
        "  for model, test_x, test_y_1 in models_testx_testy[1:]:\n",
        "    metrics, h, r = calc(model, test_x, test_y_1)\n",
        "    metrics_agg += metrics\n",
        "    ham += h\n",
        "    roc += r\n",
        "  \n",
        "  return metrics_agg/n, ham/n, roc/n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3_cIkni687i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [str(i) for i in range(1,18)]\n",
        "avg_results = metrics_avg(results, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cHrYWaEhHIv",
        "colab_type": "code",
        "outputId": "3195fb96-1705-4565-c284-d9ac328494b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "avg_results[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06598673260429105"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIaVEgrbnJ0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}