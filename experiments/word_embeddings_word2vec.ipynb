{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "word-embeddings-word2vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vondersam/sdgs_text_classifier/blob/master/experiments/word_embeddings_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBt_T5zUMG68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install iterative-stratification\n",
        "!pip install -I keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "yzkDHu1qhHIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "964c4fc3-50ab-4b54-a252-257582681fbf"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report, roc_auc_score, hamming_loss, accuracy_score\n",
        "from keras import optimizers\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "import os\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate\n",
        "from keras.models import Model\n",
        "from keras.initializers import Constant\n",
        "# Conv\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "# LSTM\n",
        "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout,SpatialDropout1D, Bidirectional\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ld3ptbEqsA",
        "colab_type": "code",
        "outputId": "f606d7e1-fa3f-4272-b04b-f7fd4b70cf16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "base_dir = \"gdrive/My Drive/fastai-v3/sdgs/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi63MqzME0Ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT_DATA_DIR = f\"{base_dir}dataset/cleanup_labelled.csv\"\n",
        "EMBEDDINGS_DIR = f\"{base_dir}embeddings/word2vec/\"\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "labels_index = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-DVSj0NhHIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(TEXT_DATA_DIR)\n",
        "df.labels = df.labels.str.split('|').apply(lambda x: [int(i) for i in x])\n",
        "vocab = Counter()\n",
        "texts = [word_tokenize(t.lower()) for t in df.text]\n",
        "for text in texts:\n",
        "    vocab.update(text)    \n",
        "model = Word2Vec(texts, size=100, window=5, min_count=5, workers=16, sg=0, negative=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRbWnT5MAcfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vectors = model.wv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7KLmdDHOvuz",
        "colab_type": "code",
        "outputId": "8736d78e-dbb2-453c-85b2-af2cc05f1571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NUM_WORDS))}\n",
        "sequences = np.array([[word_index.get(t, 0) for t in text]\n",
        "             for text in texts])\n",
        "sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "mskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels = np.array(mlb.fit_transform(df.labels))\n",
        "cross_entropy_files = []\n",
        "count = 0\n",
        "\n",
        "for train_index, val_index in mskf.split(sequences, labels):\n",
        "    count += 1\n",
        "    print(f\"Fold no. {count}\")\n",
        "    x_train = sequences[train_index]\n",
        "    x_val = sequences[val_index]\n",
        "    y_train = labels[train_index]\n",
        "    y_val = labels[val_index]\n",
        "    cross_entropy_files.append((x_train, x_val, y_train, y_val))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold no. 1\n",
            "Fold no. 2\n",
            "Fold no. 3\n",
            "Fold no. 4\n",
            "Fold no. 5\n",
            "Fold no. 6\n",
            "Fold no. 7\n",
            "Fold no. 8\n",
            "Fold no. 9\n",
            "Fold no. 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLEjjQVeSvVn",
        "colab_type": "code",
        "outputId": "f772a2a5-60af-442e-dd2f-a36a304444ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "count = 0\n",
        "results = []\n",
        "arch = 'rnn'\n",
        "for x_train, x_val, y_train, y_val in cross_entropy_files:\n",
        "    count += 1\n",
        "    print(F\"Training fold {count}\")\n",
        "    print('Preparing embedding matrix.')\n",
        "    # prepare embedding matrix\n",
        "    num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "    \n",
        "    for word, i in word_index.items():\n",
        "        if i > MAX_NUM_WORDS:\n",
        "            continue\n",
        "        try:\n",
        "            embedding_vector = word_vectors[word]\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except:\n",
        "            pass   \n",
        "    \n",
        "    # load pre-trained word embeddings into an Embedding layer\n",
        "    # note that we set trainable = False so as to keep the embeddings fixed\n",
        "    embedding_layer = Embedding(num_words,\n",
        "                                EMBEDDING_DIM,\n",
        "                                embeddings_initializer=Constant(embedding_matrix),\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=False)\n",
        "\n",
        "    print('Training model.')\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    \n",
        "    if arch == 'conv':\n",
        "        # 1D convnet with global maxpooling\n",
        "        x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "        x = MaxPooling1D(5)(x)\n",
        "        x = Conv1D(128, 5, activation='relu')(x)\n",
        "        x = MaxPooling1D(5)(x)\n",
        "        x = Conv1D(128, 5, activation='relu')(x)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        preds = Dense(len(labels_index), activation='sigmoid')(x)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='binary_crossentropy', \n",
        "                    optimizer=Adam(lr=0.01), \n",
        "                    metrics=['accuracy'])\n",
        "    \n",
        "    if arch == 'lstm':\n",
        "        # biGRU\n",
        "        embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\n",
        "        x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(embedded_sequences)\n",
        "\n",
        "        # Output\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        preds = Dense(17, activation='sigmoid')(x)\n",
        "\n",
        "        # build the model\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                    #optimizer=Adam(lr=0.0001, clipnorm=.25, beta_1=0.7, beta_2=0.99),\n",
        "                    optimizer='adam',\n",
        "                    metrics=[])\n",
        "        \n",
        "\n",
        "        x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True, dropout=0.1,\n",
        "                                                      recurrent_dropout=0.1))(x)\n",
        "    if arch == \"rnn\":\n",
        "        x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(embedded_sequences)\n",
        "\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "\n",
        "        x = concatenate([avg_pool, max_pool])\n",
        "\n",
        "        preds = Dense(17, activation=\"sigmoid\")(x)\n",
        "\n",
        "        model = Model(sequence_input, preds)\n",
        "\n",
        "        model.summary() \n",
        "\n",
        "        model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
        "\n",
        "    # Fit model    \n",
        "    model.fit(x_train, y_train,\n",
        "            batch_size=128,\n",
        "            epochs=10,\n",
        "            validation_data=(x_val, y_val))\n",
        "    results.append((model, x_val, y_val))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training fold 1\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_21 (InputLayer)           (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_21 (Embedding)        (None, 1000, 100)    1614200     input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, 998, 64)      19264       embedding_21[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_17 (Gl (None, 64)           0           conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_19 (Global (None, 64)           0           conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 128)          0           global_average_pooling1d_17[0][0]\n",
            "                                                                 global_max_pooling1d_19[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 17)           2193        concatenate_17[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 1,635,657\n",
            "Trainable params: 21,457\n",
            "Non-trainable params: 1,614,200\n",
            "__________________________________________________________________________________________________\n",
            "Train on 4711 samples, validate on 471 samples\n",
            "Epoch 1/10\n",
            "4711/4711 [==============================] - 2s 510us/step - loss: 0.3203 - acc: 0.9022 - val_loss: 0.2728 - val_acc: 0.9101\n",
            "Epoch 2/10\n",
            "4711/4711 [==============================] - 1s 168us/step - loss: 0.2344 - acc: 0.9254 - val_loss: 0.2447 - val_acc: 0.9166\n",
            "Epoch 3/10\n",
            "4711/4711 [==============================] - 1s 170us/step - loss: 0.2098 - acc: 0.9316 - val_loss: 0.2266 - val_acc: 0.9218\n",
            "Epoch 4/10\n",
            "4711/4711 [==============================] - 1s 169us/step - loss: 0.1916 - acc: 0.9358 - val_loss: 0.2095 - val_acc: 0.9261\n",
            "Epoch 5/10\n",
            "4711/4711 [==============================] - 1s 171us/step - loss: 0.1762 - acc: 0.9405 - val_loss: 0.1923 - val_acc: 0.9337\n",
            "Epoch 6/10\n",
            "4711/4711 [==============================] - 1s 170us/step - loss: 0.1654 - acc: 0.9450 - val_loss: 0.1853 - val_acc: 0.9364\n",
            "Epoch 7/10\n",
            "4711/4711 [==============================] - 1s 171us/step - loss: 0.1569 - acc: 0.9477 - val_loss: 0.1874 - val_acc: 0.9354\n",
            "Epoch 8/10\n",
            "4711/4711 [==============================] - 1s 170us/step - loss: 0.1500 - acc: 0.9495 - val_loss: 0.1814 - val_acc: 0.9401\n",
            "Epoch 9/10\n",
            "4711/4711 [==============================] - 1s 171us/step - loss: 0.1449 - acc: 0.9505 - val_loss: 0.1762 - val_acc: 0.9409\n",
            "Epoch 10/10\n",
            "4711/4711 [==============================] - 1s 174us/step - loss: 0.1397 - acc: 0.9529 - val_loss: 0.1722 - val_acc: 0.9426\n",
            "Training fold 2\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_22 (InputLayer)           (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_22 (Embedding)        (None, 1000, 100)    1614200     input_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_24 (Conv1D)              (None, 998, 64)      19264       embedding_22[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_18 (Gl (None, 64)           0           conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_20 (Global (None, 64)           0           conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 128)          0           global_average_pooling1d_18[0][0]\n",
            "                                                                 global_max_pooling1d_20[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 17)           2193        concatenate_18[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 1,635,657\n",
            "Trainable params: 21,457\n",
            "Non-trainable params: 1,614,200\n",
            "__________________________________________________________________________________________________\n",
            "Train on 4666 samples, validate on 516 samples\n",
            "Epoch 1/10\n",
            "4666/4666 [==============================] - 2s 525us/step - loss: 0.3243 - acc: 0.8983 - val_loss: 0.2572 - val_acc: 0.9179\n",
            "Epoch 2/10\n",
            "4666/4666 [==============================] - 1s 174us/step - loss: 0.2401 - acc: 0.9233 - val_loss: 0.2290 - val_acc: 0.9243\n",
            "Epoch 3/10\n",
            "4666/4666 [==============================] - 1s 170us/step - loss: 0.2101 - acc: 0.9298 - val_loss: 0.2080 - val_acc: 0.9288\n",
            "Epoch 4/10\n",
            "4666/4666 [==============================] - 1s 173us/step - loss: 0.1895 - acc: 0.9364 - val_loss: 0.1935 - val_acc: 0.9343\n",
            "Epoch 5/10\n",
            "4666/4666 [==============================] - 1s 171us/step - loss: 0.1728 - acc: 0.9410 - val_loss: 0.1876 - val_acc: 0.9379\n",
            "Epoch 6/10\n",
            "4666/4666 [==============================] - 1s 172us/step - loss: 0.1635 - acc: 0.9444 - val_loss: 0.1815 - val_acc: 0.9390\n",
            "Epoch 7/10\n",
            "4666/4666 [==============================] - 1s 172us/step - loss: 0.1559 - acc: 0.9470 - val_loss: 0.1741 - val_acc: 0.9443\n",
            "Epoch 8/10\n",
            "4666/4666 [==============================] - 1s 172us/step - loss: 0.1494 - acc: 0.9482 - val_loss: 0.1713 - val_acc: 0.9439\n",
            "Epoch 9/10\n",
            "4666/4666 [==============================] - 1s 171us/step - loss: 0.1445 - acc: 0.9512 - val_loss: 0.1653 - val_acc: 0.9449\n",
            "Epoch 10/10\n",
            "4666/4666 [==============================] - 1s 173us/step - loss: 0.1376 - acc: 0.9533 - val_loss: 0.1677 - val_acc: 0.9453\n",
            "Training fold 3\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_23 (InputLayer)           (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_23 (Embedding)        (None, 1000, 100)    1614200     input_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_25 (Conv1D)              (None, 998, 64)      19264       embedding_23[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_19 (Gl (None, 64)           0           conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_21 (Global (None, 64)           0           conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 128)          0           global_average_pooling1d_19[0][0]\n",
            "                                                                 global_max_pooling1d_21[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 17)           2193        concatenate_19[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 1,635,657\n",
            "Trainable params: 21,457\n",
            "Non-trainable params: 1,614,200\n",
            "__________________________________________________________________________________________________\n",
            "Train on 4651 samples, validate on 531 samples\n",
            "Epoch 1/10\n",
            "4651/4651 [==============================] - 2s 523us/step - loss: 0.3146 - acc: 0.9032 - val_loss: 0.2470 - val_acc: 0.9209\n",
            "Epoch 2/10\n",
            "4651/4651 [==============================] - 1s 170us/step - loss: 0.2354 - acc: 0.9238 - val_loss: 0.2135 - val_acc: 0.9294\n",
            "Epoch 3/10\n",
            "4651/4651 [==============================] - 1s 170us/step - loss: 0.2061 - acc: 0.9302 - val_loss: 0.1917 - val_acc: 0.9359\n",
            "Epoch 4/10\n",
            "4651/4651 [==============================] - 1s 173us/step - loss: 0.1894 - acc: 0.9354 - val_loss: 0.1787 - val_acc: 0.9395\n",
            "Epoch 5/10\n",
            "4651/4651 [==============================] - 1s 172us/step - loss: 0.1788 - acc: 0.9396 - val_loss: 0.1741 - val_acc: 0.9423\n",
            "Epoch 6/10\n",
            "4651/4651 [==============================] - 1s 173us/step - loss: 0.1649 - acc: 0.9433 - val_loss: 0.1681 - val_acc: 0.9443\n",
            "Epoch 7/10\n",
            "4651/4651 [==============================] - 1s 172us/step - loss: 0.1598 - acc: 0.9463 - val_loss: 0.1611 - val_acc: 0.9483\n",
            "Epoch 8/10\n",
            "4651/4651 [==============================] - 1s 171us/step - loss: 0.1503 - acc: 0.9496 - val_loss: 0.1544 - val_acc: 0.9486\n",
            "Epoch 9/10\n",
            "4651/4651 [==============================] - 1s 171us/step - loss: 0.1447 - acc: 0.9517 - val_loss: 0.1566 - val_acc: 0.9495\n",
            "Epoch 10/10\n",
            "4651/4651 [==============================] - 1s 173us/step - loss: 0.1407 - acc: 0.9527 - val_loss: 0.1543 - val_acc: 0.9488\n",
            "Training fold 4\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_24 (InputLayer)           (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_24 (Embedding)        (None, 1000, 100)    1614200     input_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_26 (Conv1D)              (None, 998, 64)      19264       embedding_24[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_20 (Gl (None, 64)           0           conv1d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_22 (Global (None, 64)           0           conv1d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 128)          0           global_average_pooling1d_20[0][0]\n",
            "                                                                 global_max_pooling1d_22[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 17)           2193        concatenate_20[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 1,635,657\n",
            "Trainable params: 21,457\n",
            "Non-trainable params: 1,614,200\n",
            "__________________________________________________________________________________________________\n",
            "Train on 4663 samples, validate on 519 samples\n",
            "Epoch 1/10\n",
            "4663/4663 [==============================] - 3s 539us/step - loss: 0.2964 - acc: 0.9091 - val_loss: 0.2424 - val_acc: 0.9242\n",
            "Epoch 2/10\n",
            "4663/4663 [==============================] - 1s 170us/step - loss: 0.2233 - acc: 0.9267 - val_loss: 0.2150 - val_acc: 0.9311\n",
            "Epoch 3/10\n",
            "4663/4663 [==============================] - 1s 170us/step - loss: 0.1957 - acc: 0.9342 - val_loss: 0.1912 - val_acc: 0.9355\n",
            "Epoch 4/10\n",
            "4663/4663 [==============================] - 1s 173us/step - loss: 0.1785 - acc: 0.9392 - val_loss: 0.1881 - val_acc: 0.9382\n",
            "Epoch 5/10\n",
            "4663/4663 [==============================] - 1s 174us/step - loss: 0.1674 - acc: 0.9428 - val_loss: 0.1739 - val_acc: 0.9429\n",
            "Epoch 6/10\n",
            "4663/4663 [==============================] - 1s 171us/step - loss: 0.1559 - acc: 0.9468 - val_loss: 0.1725 - val_acc: 0.9426\n",
            "Epoch 7/10\n",
            "4663/4663 [==============================] - 1s 177us/step - loss: 0.1481 - acc: 0.9496 - val_loss: 0.1730 - val_acc: 0.9434\n",
            "Epoch 8/10\n",
            "4663/4663 [==============================] - 1s 171us/step - loss: 0.1424 - acc: 0.9517 - val_loss: 0.1743 - val_acc: 0.9434\n",
            "Epoch 9/10\n",
            "4663/4663 [==============================] - 1s 172us/step - loss: 0.1374 - acc: 0.9530 - val_loss: 0.1736 - val_acc: 0.9438\n",
            "Epoch 10/10\n",
            "4663/4663 [==============================] - 1s 172us/step - loss: 0.1319 - acc: 0.9556 - val_loss: 0.1666 - val_acc: 0.9472\n",
            "Training fold 5\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_25 (InputLayer)           (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_25 (Embedding)        (None, 1000, 100)    1614200     input_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_27 (Conv1D)              (None, 998, 64)      19264       embedding_25[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_21 (Gl (None, 64)           0           conv1d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_23 (Global (None, 64)           0           conv1d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 128)          0           global_average_pooling1d_21[0][0]\n",
            "                                                                 global_max_pooling1d_23[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 17)           2193        concatenate_21[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 1,635,657\n",
            "Trainable params: 21,457\n",
            "Non-trainable params: 1,614,200\n",
            "__________________________________________________________________________________________________\n",
            "Train on 4670 samples, validate on 512 samples\n",
            "Epoch 1/10\n",
            "4670/4670 [==============================] - 3s 547us/step - loss: 0.3148 - acc: 0.8997 - val_loss: 0.2570 - val_acc: 0.9199\n",
            "Epoch 2/10\n",
            "4670/4670 [==============================] - 1s 170us/step - loss: 0.2337 - acc: 0.9258 - val_loss: 0.2292 - val_acc: 0.9273\n",
            "Epoch 3/10\n",
            "4670/4670 [==============================] - 1s 169us/step - loss: 0.2053 - acc: 0.9315 - val_loss: 0.1937 - val_acc: 0.9336\n",
            "Epoch 4/10\n",
            "4670/4670 [==============================] - 1s 170us/step - loss: 0.1852 - acc: 0.9377 - val_loss: 0.1830 - val_acc: 0.9364\n",
            "Epoch 5/10\n",
            "4670/4670 [==============================] - 1s 170us/step - loss: 0.1731 - acc: 0.9424 - val_loss: 0.1720 - val_acc: 0.9413\n",
            "Epoch 6/10\n",
            "4670/4670 [==============================] - 1s 171us/step - loss: 0.1638 - acc: 0.9452 - val_loss: 0.1745 - val_acc: 0.9399\n",
            "Epoch 7/10\n",
            "4670/4670 [==============================] - 1s 170us/step - loss: 0.1559 - acc: 0.9471 - val_loss: 0.1775 - val_acc: 0.9392\n",
            "Epoch 8/10\n",
            "4670/4670 [==============================] - 1s 169us/step - loss: 0.1514 - acc: 0.9490 - val_loss: 0.1629 - val_acc: 0.9463\n",
            "Epoch 9/10\n",
            "4670/4670 [==============================] - 1s 171us/step - loss: 0.1478 - acc: 0.9502 - val_loss: 0.1709 - val_acc: 0.9432\n",
            "Epoch 10/10\n",
            "4670/4670 [==============================] - 1s 172us/step - loss: 0.1416 - acc: 0.9523 - val_loss: 0.1594 - val_acc: 0.9485\n",
            "Training fold 6\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_26 (InputLayer)           (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_26 (Embedding)        (None, 1000, 100)    1614200     input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_28 (Conv1D)              (None, 998, 64)      19264       embedding_26[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_22 (Gl (None, 64)           0           conv1d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_24 (Global (None, 64)           0           conv1d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 128)          0           global_average_pooling1d_22[0][0]\n",
            "                                                                 global_max_pooling1d_24[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 17)           2193        concatenate_22[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 1,635,657\n",
            "Trainable params: 21,457\n",
            "Non-trainable params: 1,614,200\n",
            "__________________________________________________________________________________________________\n",
            "Train on 4669 samples, validate on 513 samples\n",
            "Epoch 1/10\n",
            "4669/4669 [==============================] - 3s 569us/step - loss: 0.3187 - acc: 0.9007 - val_loss: 0.2635 - val_acc: 0.9190\n",
            "Epoch 2/10\n",
            "4669/4669 [==============================] - 1s 171us/step - loss: 0.2394 - acc: 0.9232 - val_loss: 0.2335 - val_acc: 0.9252\n",
            "Epoch 3/10\n",
            "4669/4669 [==============================] - 1s 173us/step - loss: 0.2126 - acc: 0.9292 - val_loss: 0.2185 - val_acc: 0.9268\n",
            "Epoch 4/10\n",
            "4669/4669 [==============================] - 1s 172us/step - loss: 0.1956 - acc: 0.9339 - val_loss: 0.2109 - val_acc: 0.9314\n",
            "Epoch 5/10\n",
            "4669/4669 [==============================] - 1s 173us/step - loss: 0.1801 - acc: 0.9393 - val_loss: 0.1944 - val_acc: 0.9379\n",
            "Epoch 6/10\n",
            "4669/4669 [==============================] - 1s 174us/step - loss: 0.1702 - acc: 0.9420 - val_loss: 0.1932 - val_acc: 0.9369\n",
            "Epoch 7/10\n",
            "4669/4669 [==============================] - 1s 174us/step - loss: 0.1635 - acc: 0.9443 - val_loss: 0.1855 - val_acc: 0.9385\n",
            "Epoch 8/10\n",
            "4669/4669 [==============================] - 1s 171us/step - loss: 0.1558 - acc: 0.9476 - val_loss: 0.1854 - val_acc: 0.9424\n",
            "Epoch 9/10\n",
            "4669/4669 [==============================] - 1s 171us/step - loss: 0.1488 - acc: 0.9498 - val_loss: 0.1765 - val_acc: 0.9435\n",
            "Epoch 10/10\n",
            "4669/4669 [==============================] - 1s 172us/step - loss: 0.1436 - acc: 0.9508 - val_loss: 0.1817 - val_acc: 0.9416\n",
            "Training fold 7\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_27 (InputLayer)           (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_27 (Embedding)        (None, 1000, 100)    1614200     input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_29 (Conv1D)              (None, 998, 64)      19264       embedding_27[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_23 (Gl (None, 64)           0           conv1d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_25 (Global (None, 64)           0           conv1d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 128)          0           global_average_pooling1d_23[0][0]\n",
            "                                                                 global_max_pooling1d_25[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 17)           2193        concatenate_23[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 1,635,657\n",
            "Trainable params: 21,457\n",
            "Non-trainable params: 1,614,200\n",
            "__________________________________________________________________________________________________\n",
            "Train on 4655 samples, validate on 527 samples\n",
            "Epoch 1/10\n",
            "4655/4655 [==============================] - 3s 577us/step - loss: 0.3034 - acc: 0.9101 - val_loss: 0.2465 - val_acc: 0.9224\n",
            "Epoch 2/10\n",
            "4655/4655 [==============================] - 1s 172us/step - loss: 0.2333 - acc: 0.9252 - val_loss: 0.2127 - val_acc: 0.9302\n",
            "Epoch 3/10\n",
            "4655/4655 [==============================] - 1s 172us/step - loss: 0.2056 - acc: 0.9312 - val_loss: 0.1894 - val_acc: 0.9370\n",
            "Epoch 4/10\n",
            "4655/4655 [==============================] - 1s 170us/step - loss: 0.1815 - acc: 0.9394 - val_loss: 0.1765 - val_acc: 0.9416\n",
            "Epoch 5/10\n",
            "4655/4655 [==============================] - 1s 171us/step - loss: 0.1713 - acc: 0.9425 - val_loss: 0.1694 - val_acc: 0.9422\n",
            "Epoch 6/10\n",
            "4655/4655 [==============================] - 1s 171us/step - loss: 0.1634 - acc: 0.9443 - val_loss: 0.1654 - val_acc: 0.9465\n",
            "Epoch 7/10\n",
            "4655/4655 [==============================] - 1s 171us/step - loss: 0.1543 - acc: 0.9477 - val_loss: 0.1633 - val_acc: 0.9459\n",
            "Epoch 8/10\n",
            "4655/4655 [==============================] - 1s 171us/step - loss: 0.1496 - acc: 0.9491 - val_loss: 0.1617 - val_acc: 0.9466\n",
            "Epoch 9/10\n",
            "4655/4655 [==============================] - 1s 170us/step - loss: 0.1431 - acc: 0.9509 - val_loss: 0.1592 - val_acc: 0.9465\n",
            "Epoch 10/10\n",
            "4655/4655 [==============================] - 1s 170us/step - loss: 0.1403 - acc: 0.9521 - val_loss: 0.1703 - val_acc: 0.9442\n",
            "Training fold 8\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_28 (InputLayer)           (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_28 (Embedding)        (None, 1000, 100)    1614200     input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_30 (Conv1D)              (None, 998, 64)      19264       embedding_28[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_24 (Gl (None, 64)           0           conv1d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_26 (Global (None, 64)           0           conv1d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 128)          0           global_average_pooling1d_24[0][0]\n",
            "                                                                 global_max_pooling1d_26[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_30 (Dense)                (None, 17)           2193        concatenate_24[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 1,635,657\n",
            "Trainable params: 21,457\n",
            "Non-trainable params: 1,614,200\n",
            "__________________________________________________________________________________________________\n",
            "Train on 4656 samples, validate on 526 samples\n",
            "Epoch 1/10\n",
            "4656/4656 [==============================] - 3s 606us/step - loss: 0.3079 - acc: 0.9090 - val_loss: 0.2519 - val_acc: 0.9215\n",
            "Epoch 2/10\n",
            "4656/4656 [==============================] - 1s 173us/step - loss: 0.2320 - acc: 0.9259 - val_loss: 0.2207 - val_acc: 0.9265\n",
            "Epoch 3/10\n",
            "4656/4656 [==============================] - 1s 171us/step - loss: 0.2030 - acc: 0.9315 - val_loss: 0.2049 - val_acc: 0.9328\n",
            "Epoch 4/10\n",
            "4656/4656 [==============================] - 1s 172us/step - loss: 0.1829 - acc: 0.9376 - val_loss: 0.1931 - val_acc: 0.9356\n",
            "Epoch 5/10\n",
            "4656/4656 [==============================] - 1s 172us/step - loss: 0.1716 - acc: 0.9416 - val_loss: 0.1846 - val_acc: 0.9391\n",
            "Epoch 6/10\n",
            "4656/4656 [==============================] - 1s 171us/step - loss: 0.1572 - acc: 0.9465 - val_loss: 0.1785 - val_acc: 0.9420\n",
            "Epoch 7/10\n",
            "4656/4656 [==============================] - 1s 171us/step - loss: 0.1503 - acc: 0.9490 - val_loss: 0.1750 - val_acc: 0.9456\n",
            "Epoch 8/10\n",
            "4656/4656 [==============================] - 1s 172us/step - loss: 0.1419 - acc: 0.9516 - val_loss: 0.1720 - val_acc: 0.9443\n",
            "Epoch 9/10\n",
            "4656/4656 [==============================] - 1s 172us/step - loss: 0.1344 - acc: 0.9543 - val_loss: 0.1628 - val_acc: 0.9481\n",
            "Epoch 10/10\n",
            "4656/4656 [==============================] - 1s 174us/step - loss: 0.1276 - acc: 0.9571 - val_loss: 0.1747 - val_acc: 0.9455\n",
            "Training fold 9\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_29 (InputLayer)           (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_29 (Embedding)        (None, 1000, 100)    1614200     input_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_31 (Conv1D)              (None, 998, 64)      19264       embedding_29[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_25 (Gl (None, 64)           0           conv1d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_27 (Global (None, 64)           0           conv1d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 128)          0           global_average_pooling1d_25[0][0]\n",
            "                                                                 global_max_pooling1d_27[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (None, 17)           2193        concatenate_25[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 1,635,657\n",
            "Trainable params: 21,457\n",
            "Non-trainable params: 1,614,200\n",
            "__________________________________________________________________________________________________\n",
            "Train on 4656 samples, validate on 526 samples\n",
            "Epoch 1/10\n",
            "4656/4656 [==============================] - 3s 613us/step - loss: 0.3146 - acc: 0.9015 - val_loss: 0.2499 - val_acc: 0.9236\n",
            "Epoch 2/10\n",
            "4656/4656 [==============================] - 1s 173us/step - loss: 0.2344 - acc: 0.9243 - val_loss: 0.2207 - val_acc: 0.9298\n",
            "Epoch 3/10\n",
            "4656/4656 [==============================] - 1s 173us/step - loss: 0.2089 - acc: 0.9306 - val_loss: 0.2038 - val_acc: 0.9323\n",
            "Epoch 4/10\n",
            "4656/4656 [==============================] - 1s 171us/step - loss: 0.1913 - acc: 0.9350 - val_loss: 0.1901 - val_acc: 0.9376\n",
            "Epoch 5/10\n",
            "4656/4656 [==============================] - 1s 172us/step - loss: 0.1744 - acc: 0.9397 - val_loss: 0.1831 - val_acc: 0.9398\n",
            "Epoch 6/10\n",
            "4656/4656 [==============================] - 1s 172us/step - loss: 0.1666 - acc: 0.9433 - val_loss: 0.1780 - val_acc: 0.9445\n",
            "Epoch 7/10\n",
            "4656/4656 [==============================] - 1s 172us/step - loss: 0.1590 - acc: 0.9461 - val_loss: 0.1721 - val_acc: 0.9420\n",
            "Epoch 8/10\n",
            "4656/4656 [==============================] - 1s 171us/step - loss: 0.1534 - acc: 0.9478 - val_loss: 0.1792 - val_acc: 0.9387\n",
            "Epoch 9/10\n",
            "4656/4656 [==============================] - 1s 174us/step - loss: 0.1465 - acc: 0.9502 - val_loss: 0.1712 - val_acc: 0.9429\n",
            "Epoch 10/10\n",
            "4656/4656 [==============================] - 1s 171us/step - loss: 0.1425 - acc: 0.9514 - val_loss: 0.1647 - val_acc: 0.9442\n",
            "Training fold 10\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_30 (InputLayer)           (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_30 (Embedding)        (None, 1000, 100)    1614200     input_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_32 (Conv1D)              (None, 998, 64)      19264       embedding_30[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_26 (Gl (None, 64)           0           conv1d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_28 (Global (None, 64)           0           conv1d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 128)          0           global_average_pooling1d_26[0][0]\n",
            "                                                                 global_max_pooling1d_28[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_32 (Dense)                (None, 17)           2193        concatenate_26[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 1,635,657\n",
            "Trainable params: 21,457\n",
            "Non-trainable params: 1,614,200\n",
            "__________________________________________________________________________________________________\n",
            "Train on 4641 samples, validate on 541 samples\n",
            "Epoch 1/10\n",
            "4641/4641 [==============================] - 3s 630us/step - loss: 0.3072 - acc: 0.9063 - val_loss: 0.2413 - val_acc: 0.9263\n",
            "Epoch 2/10\n",
            "4641/4641 [==============================] - 1s 171us/step - loss: 0.2311 - acc: 0.9258 - val_loss: 0.2084 - val_acc: 0.9341\n",
            "Epoch 3/10\n",
            "4641/4641 [==============================] - 1s 172us/step - loss: 0.2040 - acc: 0.9317 - val_loss: 0.1933 - val_acc: 0.9377\n",
            "Epoch 4/10\n",
            "4641/4641 [==============================] - 1s 174us/step - loss: 0.1853 - acc: 0.9373 - val_loss: 0.1759 - val_acc: 0.9414\n",
            "Epoch 5/10\n",
            "4641/4641 [==============================] - 1s 172us/step - loss: 0.1762 - acc: 0.9403 - val_loss: 0.1682 - val_acc: 0.9460\n",
            "Epoch 6/10\n",
            "4641/4641 [==============================] - 1s 173us/step - loss: 0.1621 - acc: 0.9458 - val_loss: 0.1778 - val_acc: 0.9385\n",
            "Epoch 7/10\n",
            "4641/4641 [==============================] - 1s 172us/step - loss: 0.1568 - acc: 0.9476 - val_loss: 0.1635 - val_acc: 0.9469\n",
            "Epoch 8/10\n",
            "4641/4641 [==============================] - 1s 172us/step - loss: 0.1493 - acc: 0.9498 - val_loss: 0.1593 - val_acc: 0.9476\n",
            "Epoch 9/10\n",
            "4641/4641 [==============================] - 1s 172us/step - loss: 0.1421 - acc: 0.9528 - val_loss: 0.1655 - val_acc: 0.9476\n",
            "Epoch 10/10\n",
            "4641/4641 [==============================] - 1s 172us/step - loss: 0.1384 - acc: 0.9534 - val_loss: 0.1503 - val_acc: 0.9522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTa2le7E1MM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "for model, t_x, t_y in results:\n",
        "    count += 1\n",
        "    model.save(EMBEDDINGS_DIR + f\"rnn_word2vec_10_epoch-100d-10-fold-cross-val_{count}.h5\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip3GMV8X03J_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_avg(models_testx_testy, labels_, thres=0.3):\n",
        "  def calc(model, test_x, test_y):\n",
        "    predictions = model.predict(test_x)>thres\n",
        "    metrics = classification_report(test_y, predictions, target_names=labels_, output_dict=True)\n",
        "    metrics_df = pd.DataFrame.from_dict(metrics)\n",
        "    h = hamming_loss(test_y, predictions)\n",
        "    roc = roc_auc_score(test_y, predictions, average='micro')\n",
        "    return metrics_df, h, roc\n",
        "    \n",
        "  model_1, test_x_1, test_y_1 = models_testx_testy[0]\n",
        "  metrics_agg, ham, roc = calc(model_1, test_x_1, test_y_1)\n",
        "  n = len(models_testx_testy)\n",
        "  \n",
        "  for model, test_x, test_y_1 in models_testx_testy[1:]:\n",
        "    metrics, h, r = calc(model, test_x, test_y_1)\n",
        "    metrics_agg += metrics\n",
        "    ham += h\n",
        "    roc += r\n",
        "  \n",
        "  return metrics_agg/n, ham/n, roc/n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3_cIkni687i",
        "colab_type": "code",
        "outputId": "dba523ef-50e8-4a63-b03f-872ff2dadde1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "labels = [str(i) for i in range(1,18)]\n",
        "avg_results = metrics_avg(results, labels)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cHrYWaEhHIv",
        "colab_type": "code",
        "outputId": "628d1630-7439-4e3d-be3f-f44c1e2736f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "avg_results[2]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7850336276575716"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0aupM5_7xyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}