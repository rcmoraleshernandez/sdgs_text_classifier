{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "word2vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vondersam/sdgs_text_classifier/blob/master/experiments/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "yzkDHu1qhHIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report, roc_auc_score, hamming_loss, accuracy_score\n",
        "from keras import optimizers\n",
        "import os\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, Flatten\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import Constant\n",
        "# Conv\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "# LSTM\n",
        "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, SpatialDropout1D, Bidirectional, GRU, LSTM\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "\n",
        "\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ld3ptbEqsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "base_dir = \"gdrive/My Drive/fastai-v3/sdgs/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi63MqzME0Ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT_DATA_DIR = f\"{base_dir}dataset/cleanup_labelled.csv\"\n",
        "EMBEDDINGS_DIR = f\"{base_dir}embeddings/word2vec/\"\n",
        "CROSS_FOLDS = f\"{base_dir}dataset/cross_validation/\"\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 300\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "labels_index = [str(i) for i in range(1,18)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-DVSj0NhHIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(TEXT_DATA_DIR)\n",
        "labels = df.labels.str.split('|').apply(lambda x: [int(i) for i in x])\n",
        "\n",
        "### MASK\n",
        "pattern = r\"(indicator)(\\s+\\d+\\.[\\d+a-d]\\.\\d+)|(target)(\\s+\\d+\\.[\\d+a-d])|(sdgs|sdg|goals|goal)\\W*\\s+(,?\\s*\\b\\d{1,2}\\b[and\\s\\b\\d{1,2}\\b]*)\"\n",
        "masked_df = df.text.str.replace(pattern, ' SDGLABEL ', regex=True, flags=re.IGNORECASE)\n",
        "masked_df = pd.DataFrame(masked_df.str.replace('  ', ' ', regex=True, flags=re.IGNORECASE))\n",
        "\n",
        "\n",
        "vocab = Counter()\n",
        "\n",
        "\n",
        "\n",
        "# Masked for training and valid. This will be part of the vocab and index\n",
        "masked_texts = [word_tokenize(t.lower()) for t in masked_df.text]\n",
        "\n",
        "\n",
        "# Non masked for testing\n",
        "non_masked_texts = [word_tokenize(t.lower()) for t in df.text]\n",
        "\n",
        "\n",
        "# Same masked vocab, embeddings and index\n",
        "for text in texts:\n",
        "    vocab.update(text)    \n",
        "model = Word2Vec(masked_texts, size=EMBEDDING_DIM, window=5, min_count=5, workers=16, sg=0, negative=5)\n",
        "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NUM_WORDS))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRbWnT5MAcfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vectors = model.wv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7KLmdDHOvuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Masked padded sequences for training\n",
        "masked_sequences = np.array([[word_index.get(t, 0) for t in text]\n",
        "             for text in masked_texts])\n",
        "masked_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "\n",
        "# Non masked padded sequences for training\n",
        "non_masked_sequences = np.array([[word_index.get(t, 0) for t in text]\n",
        "             for text in non_masked_texts])\n",
        "non_masked_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels = np.array(mlb.fit_transform(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLEjjQVeSvVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = []\n",
        "arch = 'Conv1D_glorot_uniform'\n",
        "is_mask = \"masked\"\n",
        "\n",
        "for fold in os.listdir(CROSS_FOLDS):\n",
        "    train_index = np.load(f\"{CROSS_FOLDS}{fold}/train.npy\")\n",
        "    val_index = np.load(f\"{CROSS_FOLDS}{fold}/val.npy\")\n",
        "    test_index = np.load(f\"{CROSS_FOLDS}{fold}/test.npy\")\n",
        "\n",
        "    x_train, x_val, x_test = masked_data[train_index], masked_data[val_index], non_masked_data[test_index]\n",
        "    y_train, y_val, y_test = labels[train_index], labels[val_index], labels[test_index]\n",
        "    \n",
        "        \n",
        "    print(F\"Training {fold}\")\n",
        "\n",
        "    print('Preparing embedding matrix.')\n",
        "    # prepare embedding matrix\n",
        "    num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "    \n",
        "    for word, i in word_index.items():\n",
        "        if i > MAX_NUM_WORDS:\n",
        "            continue\n",
        "        try:\n",
        "            embedding_vector = word_vectors[word]\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except:\n",
        "            pass   \n",
        "    \n",
        "    # load pre-trained word embeddings into an Embedding layer\n",
        "    # note that we set trainable = False so as to keep the embeddings fixed\n",
        "    embedding_layer = Embedding(num_words,\n",
        "                                EMBEDDING_DIM,\n",
        "                                embeddings_initializer=Constant(embedding_matrix),\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=False)\n",
        "\n",
        "    print('Training model.')\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    # 0.22\n",
        "    if arch == 'conv': \n",
        "        # 1D convnet with global maxpooling\n",
        "        x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "        x = MaxPooling1D(5)(x)\n",
        "        x = Conv1D(128, 5, activation='relu')(x)\n",
        "        x = MaxPooling1D(5)(x)\n",
        "        x = Conv1D(128, 5, activation='relu')(x)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        preds = Dense(len(labels_index), activation='sigmoid')(x)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='binary_crossentropy', \n",
        "                    optimizer=Adam(lr=0.01), \n",
        "                    metrics=['accuracy'])\n",
        "    \n",
        "    \n",
        "    # 0.16, 8 epochs without Bidirectional\n",
        "    # 0.15, 8 epochs with Bidirectional\n",
        "    # 0.13, 10 epochs with Bidirectional\n",
        "    if arch == \"bidirectionalGRU\":\n",
        "        x = Bidirectional(GRU(128, return_sequences=True, dropout=0.1,recurrent_dropout=0.1))(embedded_sequences)\n",
        "        x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "        x = concatenate([avg_pool, max_pool])\n",
        "        preds = Dense(17, activation=\"sigmoid\")(x)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.summary() \n",
        "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    \n",
        "    # around .21, 10 epochs with Bidirectional\n",
        "    if arch == \"Bidirectional_LSTM\":\n",
        "        x = Bidirectional(LSTM(25, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedded_sequences)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dense(50, activation=\"relu\")(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(17, activation=\"sigmoid\")(x)\n",
        "        model = Model(inputs=sequence_input, outputs=x)\n",
        "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "        \n",
        "        \n",
        "    if arch == \"Conv1D_glorot_uniform\":\n",
        "        x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(embedded_sequences)\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "        x = concatenate([avg_pool, max_pool])\n",
        "        preds = Dense(len(labels_index), activation='sigmoid')(x)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='binary_crossentropy', \n",
        "                #optimizer=Adam(lr=0.001),\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(x_train, y_train,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=NUM_EPOCHS,\n",
        "            validation_data=(x_val, y_val))\n",
        "\n",
        "    models.append([model, x_test, y_test])\n",
        "    #model.save(EMBEDDINGS_DIR + f\"{is_mask}{arch}_{NUM_EPOCHS}epochs_{EMBEDDING_DIM}D_batchsize{BATCH_SIZE}_5fold-cross-val_{fold}.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPuk9pBg4yys",
        "colab_type": "text"
      },
      "source": [
        "# Load and evaluate folds on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip3GMV8X03J_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_avg(models_testx_testy, labels_, thres=0.3):\n",
        "    def calc(model, test_x, test_y):\n",
        "        predictions = model.predict(test_x)>thres\n",
        "        metrics = classification_report(test_y, predictions, target_names=labels_, output_dict=True)\n",
        "        metrics_df = pd.DataFrame.from_dict(metrics)\n",
        "        h = hamming_loss(test_y, predictions)\n",
        "        roc = roc_auc_score(test_y, predictions, average='micro')\n",
        "        return metrics_df, h, roc\n",
        "\n",
        "    model_1, test_x_first, test_y_first = models_testx_testy[0]\n",
        "    metrics_agg, ham, roc = calc(model_1, test_x_first, test_y_first)\n",
        "    n = len(models_testx_testy)\n",
        "\n",
        "    for model, test_x, test_y in models_testx_testy[1:]:\n",
        "        metrics, h, r = calc(model, test_x, test_y)\n",
        "        metrics_agg += metrics\n",
        "        ham += h\n",
        "        roc += r\n",
        "\n",
        "    return metrics_agg/n, ham/n, roc/n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tjF_N8wBHPCR",
        "colab": {}
      },
      "source": [
        "loaded_arch = 'maskedConv1D_glorot_uniform'\n",
        "loaded_models = []\n",
        "\n",
        "for i, fold in enumerate(os.listdir(CROSS_FOLDS)):\n",
        "    print(f\"Loading {fold}...\")\n",
        "    test_index = np.load(f\"{CROSS_FOLDS}{fold}/test.npy\")\n",
        "\n",
        "    x_test = data[test_index]\n",
        "    y_test = labels[test_index]\n",
        "    \n",
        "    load_dir = EMBEDDINGS_DIR + f\"{loaded_arch}_{NUM_EPOCHS}epochs_{EMBEDDING_DIM}D_batchsize{BATCH_SIZE}_5fold-cross-val_{fold}.h5\"\n",
        "    \n",
        "    final_models.append((loaded_models[i], x_test, y_test))\n",
        "print(f\"Finished loading the {loaded_arch} models.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3_cIkni687i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_results = metrics_avg(models, labels_index, thres=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t37qTPuujw6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_results[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOSBRX5Iig7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_results[0].to_csv(EMBEDDINGS_DIR + f'{is_mask}results_{arch}.csv', sep=';')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cHrYWaEhHIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hl = round(avg_results[1],4)\n",
        "roc_auc = round(avg_results[2],4)\n",
        "print(f\"hl;{hl}\")\n",
        "print(f\"roc-auc;{roc_auc}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHCwDRXIkA93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}