{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "word2vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vondersam/sdgs_text_classifier/blob/master/experiments/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "yzkDHu1qhHIa",
        "colab_type": "code",
        "outputId": "3f517a39-16eb-4ca8-dee8-5f9d5447b065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report, roc_auc_score, hamming_loss, accuracy_score\n",
        "from keras import optimizers\n",
        "import os\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, Flatten\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import Constant\n",
        "# Conv\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "# LSTM\n",
        "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, SpatialDropout1D, Bidirectional, GRU, LSTM\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from keras.models import load_model"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ld3ptbEqsA",
        "colab_type": "code",
        "outputId": "d2118485-9c87-40ea-b2d6-2f50891950d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "base_dir = \"gdrive/My Drive/fastai-v3/sdgs/\""
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi63MqzME0Ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT_DATA_DIR = f\"{base_dir}dataset/cleanup_labelled.csv\"\n",
        "EMBEDDINGS_DIR = f\"{base_dir}embeddings/word2vec/\"\n",
        "CROSS_FOLDS = f\"{base_dir}dataset/cross_validation/\"\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 300\n",
        "NUM_EPOCHS = 15\n",
        "BATCH_SIZE = 128\n",
        "labels_index = [str(i) for i in range(1,18)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-DVSj0NhHIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(TEXT_DATA_DIR)\n",
        "df.labels = df.labels.str.split('|').apply(lambda x: [int(i) for i in x])\n",
        "\n",
        "vocab = Counter()\n",
        "texts = [word_tokenize(t.lower()) for t in df.text]\n",
        "\n",
        "for text in texts:\n",
        "    vocab.update(text)    \n",
        "\n",
        "model = Word2Vec(texts, size=EMBEDDING_DIM, window=5, min_count=5, workers=16, sg=0, negative=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRbWnT5MAcfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vectors = model.wv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7KLmdDHOvuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NUM_WORDS))}\n",
        "sequences = np.array([[word_index.get(t, 0) for t in text]\n",
        "             for text in texts])\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels = np.array(mlb.fit_transform(df.labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLEjjQVeSvVn",
        "colab_type": "code",
        "outputId": "4aa426fa-0058-445f-e02e-dd9b3d13ed7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "results = []\n",
        "arch = 'Conv1D_glorot_uniform'\n",
        "\n",
        "for fold in os.listdir(CROSS_FOLDS):\n",
        "    train_index = np.load(f\"{CROSS_FOLDS}{fold}/train.npy\")\n",
        "    val_index = np.load(f\"{CROSS_FOLDS}{fold}/val.npy\")\n",
        "    #test_index = np.load(f\"{CROSS_FOLDS}{fold}/test.npy\")\n",
        "\n",
        "    x_train, x_val = data[train_index], data[val_index]\n",
        "    y_train, y_val = labels[train_index], labels[val_index]\n",
        "    \n",
        "        \n",
        "    print(F\"Training {fold}\")\n",
        "\n",
        "    print('Preparing embedding matrix.')\n",
        "    # prepare embedding matrix\n",
        "    num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "    \n",
        "    for word, i in word_index.items():\n",
        "        if i > MAX_NUM_WORDS:\n",
        "            continue\n",
        "        try:\n",
        "            embedding_vector = word_vectors[word]\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except:\n",
        "            pass   \n",
        "    \n",
        "    # load pre-trained word embeddings into an Embedding layer\n",
        "    # note that we set trainable = False so as to keep the embeddings fixed\n",
        "    embedding_layer = Embedding(num_words,\n",
        "                                EMBEDDING_DIM,\n",
        "                                embeddings_initializer=Constant(embedding_matrix),\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=False)\n",
        "\n",
        "    print('Training model.')\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    # 0.22\n",
        "    if arch == 'conv': \n",
        "        # 1D convnet with global maxpooling\n",
        "        x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "        x = MaxPooling1D(5)(x)\n",
        "        x = Conv1D(128, 5, activation='relu')(x)\n",
        "        x = MaxPooling1D(5)(x)\n",
        "        x = Conv1D(128, 5, activation='relu')(x)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        preds = Dense(len(labels_index), activation='sigmoid')(x)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='binary_crossentropy', \n",
        "                    optimizer=Adam(lr=0.01), \n",
        "                    metrics=['accuracy'])\n",
        "    \n",
        "    \n",
        "    # 0.16, 8 epochs without Bidirectional\n",
        "    # 0.15, 8 epochs with Bidirectional\n",
        "    # 0.13, 10 epochs with Bidirectional\n",
        "    if arch == \"bidirectionalGRU\":\n",
        "        x = Bidirectional(GRU(128, return_sequences=True, dropout=0.1,recurrent_dropout=0.1))(embedded_sequences)\n",
        "        x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "        x = concatenate([avg_pool, max_pool])\n",
        "        preds = Dense(17, activation=\"sigmoid\")(x)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.summary() \n",
        "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    \n",
        "    # around .21, 10 epochs with Bidirectional\n",
        "    if arch == \"Bidirectional_LSTM\":\n",
        "        x = Bidirectional(LSTM(25, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedded_sequences)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dense(50, activation=\"relu\")(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(17, activation=\"sigmoid\")(x)\n",
        "        model = Model(inputs=sequence_input, outputs=x)\n",
        "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "        \n",
        "        \n",
        "    if arch == \"Conv1D_glorot_uniform\":\n",
        "        x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(embedded_sequences)\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "        x = concatenate([avg_pool, max_pool])\n",
        "        preds = Dense(len(labels_index), activation='sigmoid')(x)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='binary_crossentropy', \n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(x_train, y_train,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=NUM_EPOCHS,\n",
        "            validation_data=(x_val, y_val))\n",
        "    \n",
        "    model.save(EMBEDDINGS_DIR + f\"{arch}_{NUM_EPOCHS}epochs_{EMBEDDING_DIM}D_batchsize{BATCH_SIZE}_5fold-cross-val_{fold}.h5\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training fold_1\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "Train on 4173 samples, validate on 516 samples\n",
            "Epoch 1/15\n",
            "4173/4173 [==============================] - 16s 4ms/step - loss: 0.3238 - acc: 0.8958 - val_loss: 0.2611 - val_acc: 0.9204\n",
            "Epoch 2/15\n",
            "4173/4173 [==============================] - 2s 362us/step - loss: 0.2493 - acc: 0.9212 - val_loss: 0.2336 - val_acc: 0.9248\n",
            "Epoch 3/15\n",
            "4173/4173 [==============================] - 2s 373us/step - loss: 0.2262 - acc: 0.9248 - val_loss: 0.2207 - val_acc: 0.9276\n",
            "Epoch 4/15\n",
            "4173/4173 [==============================] - 2s 372us/step - loss: 0.2118 - acc: 0.9274 - val_loss: 0.2064 - val_acc: 0.9301\n",
            "Epoch 5/15\n",
            "4173/4173 [==============================] - 2s 370us/step - loss: 0.2016 - acc: 0.9299 - val_loss: 0.1993 - val_acc: 0.9317\n",
            "Epoch 6/15\n",
            "4173/4173 [==============================] - 2s 373us/step - loss: 0.1932 - acc: 0.9325 - val_loss: 0.1982 - val_acc: 0.9346\n",
            "Epoch 7/15\n",
            "4173/4173 [==============================] - 2s 369us/step - loss: 0.1869 - acc: 0.9346 - val_loss: 0.1908 - val_acc: 0.9333\n",
            "Epoch 8/15\n",
            "4173/4173 [==============================] - 2s 380us/step - loss: 0.1817 - acc: 0.9367 - val_loss: 0.1865 - val_acc: 0.9352\n",
            "Epoch 9/15\n",
            "4173/4173 [==============================] - 2s 387us/step - loss: 0.1764 - acc: 0.9391 - val_loss: 0.1861 - val_acc: 0.9355\n",
            "Epoch 10/15\n",
            "4173/4173 [==============================] - 2s 375us/step - loss: 0.1729 - acc: 0.9404 - val_loss: 0.1830 - val_acc: 0.9360\n",
            "Epoch 11/15\n",
            "4173/4173 [==============================] - 2s 389us/step - loss: 0.1686 - acc: 0.9417 - val_loss: 0.1834 - val_acc: 0.9395\n",
            "Epoch 12/15\n",
            "4173/4173 [==============================] - 2s 385us/step - loss: 0.1657 - acc: 0.9432 - val_loss: 0.1833 - val_acc: 0.9403\n",
            "Epoch 13/15\n",
            "4173/4173 [==============================] - 2s 398us/step - loss: 0.1624 - acc: 0.9443 - val_loss: 0.1756 - val_acc: 0.9394\n",
            "Epoch 14/15\n",
            "4173/4173 [==============================] - 2s 398us/step - loss: 0.1598 - acc: 0.9455 - val_loss: 0.1711 - val_acc: 0.9424\n",
            "Epoch 15/15\n",
            "4173/4173 [==============================] - 2s 382us/step - loss: 0.1568 - acc: 0.9465 - val_loss: 0.1766 - val_acc: 0.9388\n",
            "Training fold_2\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "Train on 4151 samples, validate on 514 samples\n",
            "Epoch 1/15\n",
            "4151/4151 [==============================] - 16s 4ms/step - loss: 0.3215 - acc: 0.9001 - val_loss: 0.2661 - val_acc: 0.9201\n",
            "Epoch 2/15\n",
            "4151/4151 [==============================] - 2s 384us/step - loss: 0.2517 - acc: 0.9204 - val_loss: 0.2434 - val_acc: 0.9205\n",
            "Epoch 3/15\n",
            "4151/4151 [==============================] - 2s 404us/step - loss: 0.2298 - acc: 0.9242 - val_loss: 0.2264 - val_acc: 0.9273\n",
            "Epoch 4/15\n",
            "4151/4151 [==============================] - 2s 379us/step - loss: 0.2149 - acc: 0.9275 - val_loss: 0.2152 - val_acc: 0.9292\n",
            "Epoch 5/15\n",
            "4151/4151 [==============================] - 2s 377us/step - loss: 0.2043 - acc: 0.9299 - val_loss: 0.2080 - val_acc: 0.9292\n",
            "Epoch 6/15\n",
            "4151/4151 [==============================] - 1s 351us/step - loss: 0.1956 - acc: 0.9322 - val_loss: 0.2002 - val_acc: 0.9302\n",
            "Epoch 7/15\n",
            "4151/4151 [==============================] - 2s 362us/step - loss: 0.1891 - acc: 0.9339 - val_loss: 0.1953 - val_acc: 0.9331\n",
            "Epoch 8/15\n",
            "4151/4151 [==============================] - 2s 366us/step - loss: 0.1834 - acc: 0.9357 - val_loss: 0.1889 - val_acc: 0.9351\n",
            "Epoch 9/15\n",
            "4151/4151 [==============================] - 2s 370us/step - loss: 0.1775 - acc: 0.9381 - val_loss: 0.1869 - val_acc: 0.9353\n",
            "Epoch 10/15\n",
            "4151/4151 [==============================] - 2s 373us/step - loss: 0.1733 - acc: 0.9394 - val_loss: 0.1916 - val_acc: 0.9359\n",
            "Epoch 11/15\n",
            "4151/4151 [==============================] - 2s 377us/step - loss: 0.1692 - acc: 0.9417 - val_loss: 0.1864 - val_acc: 0.9390\n",
            "Epoch 12/15\n",
            "4151/4151 [==============================] - 2s 377us/step - loss: 0.1651 - acc: 0.9426 - val_loss: 0.1828 - val_acc: 0.9385\n",
            "Epoch 13/15\n",
            "4151/4151 [==============================] - 2s 371us/step - loss: 0.1623 - acc: 0.9441 - val_loss: 0.1758 - val_acc: 0.9415\n",
            "Epoch 14/15\n",
            "4151/4151 [==============================] - 2s 390us/step - loss: 0.1582 - acc: 0.9459 - val_loss: 0.1758 - val_acc: 0.9419\n",
            "Epoch 15/15\n",
            "4151/4151 [==============================] - 2s 368us/step - loss: 0.1563 - acc: 0.9463 - val_loss: 0.1793 - val_acc: 0.9403\n",
            "Training fold_3\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "Train on 4142 samples, validate on 513 samples\n",
            "Epoch 1/15\n",
            "4142/4142 [==============================] - 16s 4ms/step - loss: 0.3176 - acc: 0.8973 - val_loss: 0.2464 - val_acc: 0.9265\n",
            "Epoch 2/15\n",
            "4142/4142 [==============================] - 2s 365us/step - loss: 0.2508 - acc: 0.9199 - val_loss: 0.2188 - val_acc: 0.9318\n",
            "Epoch 3/15\n",
            "4142/4142 [==============================] - 2s 387us/step - loss: 0.2292 - acc: 0.9238 - val_loss: 0.2086 - val_acc: 0.9349\n",
            "Epoch 4/15\n",
            "4142/4142 [==============================] - 2s 373us/step - loss: 0.2141 - acc: 0.9261 - val_loss: 0.1977 - val_acc: 0.9350\n",
            "Epoch 5/15\n",
            "4142/4142 [==============================] - 2s 382us/step - loss: 0.2041 - acc: 0.9294 - val_loss: 0.1892 - val_acc: 0.9367\n",
            "Epoch 6/15\n",
            "4142/4142 [==============================] - 2s 369us/step - loss: 0.1953 - acc: 0.9316 - val_loss: 0.2021 - val_acc: 0.9377\n",
            "Epoch 7/15\n",
            "4142/4142 [==============================] - 2s 382us/step - loss: 0.1896 - acc: 0.9338 - val_loss: 0.1835 - val_acc: 0.9413\n",
            "Epoch 8/15\n",
            "4142/4142 [==============================] - 2s 379us/step - loss: 0.1841 - acc: 0.9358 - val_loss: 0.1801 - val_acc: 0.9398\n",
            "Epoch 9/15\n",
            "4142/4142 [==============================] - 1s 358us/step - loss: 0.1790 - acc: 0.9378 - val_loss: 0.1823 - val_acc: 0.9393\n",
            "Epoch 10/15\n",
            "4142/4142 [==============================] - 1s 345us/step - loss: 0.1745 - acc: 0.9395 - val_loss: 0.1774 - val_acc: 0.9412\n",
            "Epoch 11/15\n",
            "4142/4142 [==============================] - 1s 352us/step - loss: 0.1710 - acc: 0.9408 - val_loss: 0.1832 - val_acc: 0.9408\n",
            "Epoch 12/15\n",
            "4142/4142 [==============================] - 2s 370us/step - loss: 0.1678 - acc: 0.9414 - val_loss: 0.1704 - val_acc: 0.9436\n",
            "Epoch 13/15\n",
            "4142/4142 [==============================] - 2s 383us/step - loss: 0.1651 - acc: 0.9428 - val_loss: 0.1673 - val_acc: 0.9453\n",
            "Epoch 14/15\n",
            "4142/4142 [==============================] - 2s 367us/step - loss: 0.1618 - acc: 0.9441 - val_loss: 0.1652 - val_acc: 0.9455\n",
            "Epoch 15/15\n",
            "4142/4142 [==============================] - 2s 373us/step - loss: 0.1594 - acc: 0.9448 - val_loss: 0.1662 - val_acc: 0.9463\n",
            "Training fold_4\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "Train on 4140 samples, validate on 512 samples\n",
            "Epoch 1/15\n",
            "4140/4140 [==============================] - 16s 4ms/step - loss: 0.3279 - acc: 0.8931 - val_loss: 0.2613 - val_acc: 0.9236\n",
            "Epoch 2/15\n",
            "4140/4140 [==============================] - 1s 362us/step - loss: 0.2538 - acc: 0.9194 - val_loss: 0.2374 - val_acc: 0.9265\n",
            "Epoch 3/15\n",
            "4140/4140 [==============================] - 2s 370us/step - loss: 0.2322 - acc: 0.9233 - val_loss: 0.2212 - val_acc: 0.9272\n",
            "Epoch 4/15\n",
            "4140/4140 [==============================] - 2s 367us/step - loss: 0.2174 - acc: 0.9261 - val_loss: 0.2128 - val_acc: 0.9314\n",
            "Epoch 5/15\n",
            "4140/4140 [==============================] - 1s 358us/step - loss: 0.2070 - acc: 0.9296 - val_loss: 0.2024 - val_acc: 0.9326\n",
            "Epoch 6/15\n",
            "4140/4140 [==============================] - 1s 354us/step - loss: 0.1988 - acc: 0.9317 - val_loss: 0.2012 - val_acc: 0.9338\n",
            "Epoch 7/15\n",
            "4140/4140 [==============================] - 1s 356us/step - loss: 0.1917 - acc: 0.9335 - val_loss: 0.1930 - val_acc: 0.9343\n",
            "Epoch 8/15\n",
            "4140/4140 [==============================] - 2s 385us/step - loss: 0.1855 - acc: 0.9355 - val_loss: 0.1891 - val_acc: 0.9349\n",
            "Epoch 9/15\n",
            "4140/4140 [==============================] - 2s 373us/step - loss: 0.1808 - acc: 0.9371 - val_loss: 0.1888 - val_acc: 0.9370\n",
            "Epoch 10/15\n",
            "4140/4140 [==============================] - 2s 369us/step - loss: 0.1761 - acc: 0.9389 - val_loss: 0.1819 - val_acc: 0.9386\n",
            "Epoch 11/15\n",
            "4140/4140 [==============================] - 2s 372us/step - loss: 0.1724 - acc: 0.9405 - val_loss: 0.1818 - val_acc: 0.9374\n",
            "Epoch 12/15\n",
            "4140/4140 [==============================] - 2s 382us/step - loss: 0.1686 - acc: 0.9417 - val_loss: 0.1818 - val_acc: 0.9383\n",
            "Epoch 13/15\n",
            "4140/4140 [==============================] - 2s 375us/step - loss: 0.1661 - acc: 0.9429 - val_loss: 0.1758 - val_acc: 0.9395\n",
            "Epoch 14/15\n",
            "4140/4140 [==============================] - 1s 362us/step - loss: 0.1627 - acc: 0.9439 - val_loss: 0.1758 - val_acc: 0.9418\n",
            "Epoch 15/15\n",
            "4140/4140 [==============================] - 2s 401us/step - loss: 0.1596 - acc: 0.9453 - val_loss: 0.1720 - val_acc: 0.9407\n",
            "Training fold_5\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "Train on 4155 samples, validate on 514 samples\n",
            "Epoch 1/15\n",
            "4155/4155 [==============================] - 17s 4ms/step - loss: 0.3025 - acc: 0.9076 - val_loss: 0.2654 - val_acc: 0.9185\n",
            "Epoch 2/15\n",
            "4155/4155 [==============================] - 1s 354us/step - loss: 0.2451 - acc: 0.9209 - val_loss: 0.2397 - val_acc: 0.9209\n",
            "Epoch 3/15\n",
            "4155/4155 [==============================] - 2s 369us/step - loss: 0.2246 - acc: 0.9246 - val_loss: 0.2264 - val_acc: 0.9277\n",
            "Epoch 4/15\n",
            "4155/4155 [==============================] - 1s 360us/step - loss: 0.2120 - acc: 0.9277 - val_loss: 0.2143 - val_acc: 0.9277\n",
            "Epoch 5/15\n",
            "4155/4155 [==============================] - 2s 370us/step - loss: 0.2019 - acc: 0.9298 - val_loss: 0.2084 - val_acc: 0.9327\n",
            "Epoch 6/15\n",
            "4155/4155 [==============================] - 1s 356us/step - loss: 0.1943 - acc: 0.9328 - val_loss: 0.2036 - val_acc: 0.9317\n",
            "Epoch 7/15\n",
            "4155/4155 [==============================] - 2s 362us/step - loss: 0.1879 - acc: 0.9347 - val_loss: 0.1972 - val_acc: 0.9331\n",
            "Epoch 8/15\n",
            "4155/4155 [==============================] - 2s 366us/step - loss: 0.1817 - acc: 0.9366 - val_loss: 0.1911 - val_acc: 0.9363\n",
            "Epoch 9/15\n",
            "4155/4155 [==============================] - 2s 384us/step - loss: 0.1768 - acc: 0.9389 - val_loss: 0.1902 - val_acc: 0.9355\n",
            "Epoch 10/15\n",
            "4155/4155 [==============================] - 2s 383us/step - loss: 0.1718 - acc: 0.9403 - val_loss: 0.1861 - val_acc: 0.9380\n",
            "Epoch 11/15\n",
            "4155/4155 [==============================] - 2s 374us/step - loss: 0.1688 - acc: 0.9412 - val_loss: 0.1830 - val_acc: 0.9381\n",
            "Epoch 12/15\n",
            "4155/4155 [==============================] - 2s 380us/step - loss: 0.1647 - acc: 0.9433 - val_loss: 0.1838 - val_acc: 0.9403\n",
            "Epoch 13/15\n",
            "4155/4155 [==============================] - 2s 389us/step - loss: 0.1612 - acc: 0.9443 - val_loss: 0.1797 - val_acc: 0.9392\n",
            "Epoch 14/15\n",
            "4155/4155 [==============================] - 2s 382us/step - loss: 0.1581 - acc: 0.9454 - val_loss: 0.1750 - val_acc: 0.9407\n",
            "Epoch 15/15\n",
            "4155/4155 [==============================] - 2s 387us/step - loss: 0.1555 - acc: 0.9464 - val_loss: 0.1773 - val_acc: 0.9404\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPuk9pBg4yys",
        "colab_type": "text"
      },
      "source": [
        "# Load and evaluate folds on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip3GMV8X03J_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_avg(models_testx_testy, labels_, thres=0.3):\n",
        "    def calc(model, test_x, test_y):\n",
        "        predictions = model.predict(test_x)>thres\n",
        "        metrics = classification_report(test_y, predictions, target_names=labels_, output_dict=True)\n",
        "        metrics_df = pd.DataFrame.from_dict(metrics)\n",
        "        h = hamming_loss(test_y, predictions)\n",
        "        roc = roc_auc_score(test_y, predictions, average='micro')\n",
        "        return metrics_df, h, roc\n",
        "\n",
        "    model_1, test_x_first, test_y_first = models_testx_testy[0]\n",
        "    metrics_agg, ham, roc = calc(model_1, test_x_first, test_y_first)\n",
        "    n = len(models_testx_testy)\n",
        "\n",
        "    for model, test_x, test_y in models_testx_testy[1:]:\n",
        "        metrics, h, r = calc(model, test_x, test_y)\n",
        "        metrics_agg += metrics\n",
        "        ham += h\n",
        "        roc += r\n",
        "\n",
        "    return metrics_agg/n, ham/n, roc/n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JihY-XG144EK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "f3a82fab-beb3-47ed-9e47-5123685503c9"
      },
      "source": [
        "loaded_arch = 'Conv1D_glorot_uniform'\n",
        "loaded_models = []\n",
        "for fold in os.listdir(CROSS_FOLDS):\n",
        "    print(f\"Loading {fold}...\")\n",
        "    test_index = np.load(f\"{CROSS_FOLDS}{fold}/test.npy\")\n",
        "\n",
        "    x_test = data[test_index]\n",
        "    y_test = labels[test_index]\n",
        "    \n",
        "    load_dir = EMBEDDINGS_DIR + f\"{loaded_arch}_{NUM_EPOCHS}epochs_{EMBEDDING_DIM}D_batchsize{BATCH_SIZE}_5fold-cross-val_{fold}.h5\"\n",
        "    loaded_model = load_model(load_dir)\n",
        "    \n",
        "    loaded_models.append((loaded_model, x_test, y_test))\n",
        "print(f\"Finished loading the {loaded_arch} models.\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading fold_1...\n",
            "Loading fold_2...\n",
            "Loading fold_3...\n",
            "Loading fold_4...\n",
            "Loading fold_5...\n",
            "Finished loading the Conv1D_glorot_uniform models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3_cIkni687i",
        "colab_type": "code",
        "outputId": "a3d410db-e54c-45a3-bc38-e8be1c4f3773",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "avg_results = metrics_avg(loaded_models, labels_index, thres=0.2)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cHrYWaEhHIv",
        "colab_type": "code",
        "outputId": "2c5901f5-657d-4e51-e495-312a0e700d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "avg_results[2]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7739318542452324"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0aupM5_7xyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}