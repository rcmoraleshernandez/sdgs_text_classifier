{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "glove.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vondersam/sdgs_text_classifier/blob/master/experiments/glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "yzkDHu1qhHIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report, roc_auc_score, hamming_loss, accuracy_score\n",
        "from keras import optimizers\n",
        "import os\n",
        "import re\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, Bidirectional\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, LSTM, Conv1D, GlobalMaxPooling1D, CuDNNLSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "#from tensorflow.keras.backend import set_session\n",
        "#sess = tf.Session()\n",
        "#set_session(sess)\n",
        "#sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ld3ptbEqsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "base_dir = \"gdrive/My Drive/fastai-v3/sdgs/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi63MqzME0Ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT_DATA_DIR = f\"{base_dir}dataset/cleanup_labelled.csv\"\n",
        "CROSS_FOLDS = f\"{base_dir}dataset/cross_validation/\"\n",
        "GLOVE_DIR = f\"{base_dir}embeddings/glove/glove.6B/\"\n",
        "EMBEDDINGS_DIR = f\"{base_dir}embeddings/glove/\"\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 300\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "labels_index = [str(i) for i in range(1,18)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-DVSj0NhHIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pretrained embeddings in an index mapping words in the embeddings set\n",
        "# to their embeddings vector\n",
        "print('Indexing word vectors.')\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt')) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilVT-WLTmmH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "is_mask = \"\"\n",
        "df = pd.read_csv(TEXT_DATA_DIR)\n",
        "\n",
        "###### MASK LABELS\n",
        "pattern = r\"(indicator)(\\s+\\d+\\.[\\d+a-d]\\.\\d+)|(target)(\\s+\\d+\\.[\\d+a-d])|(sdgs|sdg|goals|goal)\\W*\\s+(,?\\s*\\b\\d{1,2}\\b[and\\s\\b\\d{1,2}\\b]*)\"\n",
        "masked_df = df.text.str.replace(pattern, ' SDGLABEL ', regex=True, flags=re.IGNORECASE)\n",
        "masked_df = pd.DataFrame(masked_df.str.replace('  ', ' ', regex=True, flags=re.IGNORECASE))\n",
        "\n",
        "\n",
        "# Masked sequences for training, word index \n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(masked_df.text)\n",
        "word_index = tokenizer.word_index\n",
        "masked_sequences = tokenizer.texts_to_sequences(masked_df.text)\n",
        "masked_data = pad_sequences(masked_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Non masked sequences for testing\n",
        "non_masked_sequences = tokenizer.texts_to_sequences(df.text)\n",
        "non_masked_data = pad_sequences(non_masked_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Labels\n",
        "mlb = MultiLabelBinarizer()\n",
        "df.labels = df.labels.str.split('|').apply(lambda x: [int(i) for i in x])\n",
        "labels = mlb.fit_transform(df.labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLEjjQVeSvVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = []\n",
        "arch = 'Conv1D_glorot_uniform'\n",
        "\n",
        "# Cross-validation: split the data into a training set and a test set\n",
        "for fold in os.listdir(CROSS_FOLDS):\n",
        "    train_index = np.load(f\"{CROSS_FOLDS}{fold}/train.npy\")\n",
        "    val_index = np.load(f\"{CROSS_FOLDS}{fold}/val.npy\")\n",
        "    test_index = np.load(f\"{CROSS_FOLDS}{fold}/test.npy\")\n",
        "\n",
        "    # Masked for training, and non_masked for testing\n",
        "    x_train, x_val, x_test  = masked_data[train_index], masked_data[val_index], non_masked_data[test_index]\n",
        "    y_train, y_val, y_test = labels[train_index], labels[val_index], labels[test_index]\n",
        "    \n",
        "    print(fold)\n",
        "    print('Preparing embedding matrix.')\n",
        "    \n",
        "    # prepare embedding matrix\n",
        "    num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        # Ignore word if not in the n most common words\n",
        "        if i > MAX_NUM_WORDS:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    # load pre-trained word embeddings into an Embedding layer\n",
        "    # note that we set trainable = False so as to keep the embeddings fixed\n",
        "    embedding_layer = Embedding(num_words,\n",
        "                                EMBEDDING_DIM,\n",
        "                                embeddings_initializer=Constant(embedding_matrix),\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=False)\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    \n",
        "    \n",
        "    # \n",
        "    if arch == \"Bidirectional_LSTM\":\n",
        "        x = Bidirectional(LSTM(25, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedded_sequences)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dense(50, activation=\"relu\")(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(17, activation=\"sigmoid\")(x)\n",
        "        model = Model(inputs=sequence_input, outputs=x)\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    \n",
        "    # 0.179 with 10 epochs, 300 dimensions\n",
        "    if arch == \"convnet\":\n",
        "        # 1D convnet with global maxpooling\n",
        "        x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "        x = MaxPooling1D(5)(x)\n",
        "        x = Conv1D(128, 5, activation='relu')(x)\n",
        "        x = MaxPooling1D(5)(x)\n",
        "        x = Conv1D(128, 5, activation='relu')(x)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        preds = Dense(len(labels_index), activation='sigmoid')(x)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='binary_crossentropy', \n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy'])\n",
        "        \n",
        "    \n",
        "    # 0.131 with 20 epochs, 300 dimensions\n",
        "    if arch == \"Conv1D_glorot_uniform\":\n",
        "        x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(embedded_sequences)\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "        x = concatenate([avg_pool, max_pool])\n",
        "        preds = Dense(len(labels_index), activation='sigmoid')(x)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='binary_crossentropy', \n",
        "                optimizer=RMSprop(lr=0.001),\n",
        "                metrics=['accuracy'])\n",
        "   \n",
        "\n",
        "        \n",
        "    if arch == \"convolution1d\":\n",
        "        #https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py\n",
        "        model = Sequential()\n",
        "\n",
        "        # we start off with an efficient embedding layer which maps\n",
        "        # our vocab indices into embedding_dims dimensions\n",
        "        #model.add(embedded_sequences)\n",
        "        model.add(Embedding(num_words,\n",
        "                    EMBEDDING_DIM,\n",
        "                    input_length=MAX_SEQUENCE_LENGTH))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        # we add a Convolution1D, which will learn filters\n",
        "        # word group filters of size filter_length:\n",
        "        model.add(Conv1D(filters,\n",
        "                         kernel_size,\n",
        "                         padding='valid',\n",
        "                         activation='relu',\n",
        "                         strides=1))\n",
        "        # we use max pooling:\n",
        "        model.add(GlobalMaxPooling1D())\n",
        "\n",
        "        # We add a vanilla hidden layer:\n",
        "        model.add(Dense(hidden_dims))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "        # We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "        model.add(len(labels_index))\n",
        "        model.add(Activation('sigmoid'))\n",
        "\n",
        "\n",
        "    model.fit(x_train, y_train,\n",
        "            batch_size=128,\n",
        "            epochs=NUM_EPOCHS,\n",
        "            validation_data=(x_val, y_val))\n",
        "\n",
        "    models.append((model, x_test, y_test))\n",
        "    #model.save(EMBEDDINGS_DIR + f\"{is_mask}{arch}_{NUM_EPOCHS}epochs_{EMBEDDING_DIM}D_batchsize{BATCH_SIZE}_5fold-cross-val_{fold}.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dnhmTNH7KeM",
        "colab_type": "text"
      },
      "source": [
        "# Load and evaluate folds on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip3GMV8X03J_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_avg(models_testx_testy, labels_, thres=0.3):\n",
        "    def calc(model, test_x, test_y):\n",
        "        predictions = model.predict(test_x)>thres\n",
        "        metrics = classification_report(test_y, predictions, target_names=labels_, output_dict=True)\n",
        "        metrics_df = pd.DataFrame.from_dict(metrics)\n",
        "        h = hamming_loss(test_y, predictions)\n",
        "        roc = roc_auc_score(test_y, predictions, average='micro')\n",
        "        return metrics_df, h, roc\n",
        "\n",
        "    model_1, test_x_first, test_y_first = models_testx_testy[0]\n",
        "    metrics_agg, ham, roc = calc(model_1, test_x_first, test_y_first)\n",
        "    n = len(models_testx_testy)\n",
        "\n",
        "    for model, test_x, test_y in models_testx_testy[1:]:\n",
        "        metrics, h, r = calc(model, test_x, test_y)\n",
        "        metrics_agg += metrics\n",
        "        ham += h\n",
        "        roc += r\n",
        "\n",
        "    return metrics_agg/n, ham/n, roc/n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVBmo_HUH7O5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_arch = 'Conv1D_glorot_uniform'\n",
        "loaded_models = []\n",
        "for fold in os.listdir(CROSS_FOLDS):\n",
        "    print(f\"Loading {fold}...\")\n",
        "    test_index = np.load(f\"{CROSS_FOLDS}{fold}/test.npy\")\n",
        "\n",
        "    x_test = data[test_index]\n",
        "    y_test = labels[test_index]\n",
        "    \n",
        "    load_dir = EMBEDDINGS_DIR + f\"{loaded_arch}_{NUM_EPOCHS}epochs_{EMBEDDING_DIM}D_batchsize{BATCH_SIZE}_5fold-cross-val_{fold}.h5\"\n",
        "    loaded_model = load_model(load_dir)\n",
        "    \n",
        "    loaded_models.append((loaded_model, x_test, y_test))\n",
        "print(f\"Finished loading the {loaded_arch} models.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYVLRfpK7QiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_results = metrics_avg(models, labels_index); avg_results[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLr3rO4g8mDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#avg_results[0].to_csv(EMBEDDINGS_DIR + f'masked_results_{arch}.csv', sep=';')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiZRbVe9o6b9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hl = round(avg_results[1],4)\n",
        "roc_auc = round(avg_results[2],4)\n",
        "print(f\"hl;{hl};;roc-auc;{roc_auc}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swnzksFiU6x5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}