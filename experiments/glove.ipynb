{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "glove.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vondersam/sdgs_text_classifier/blob/master/experiments/glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "yzkDHu1qhHIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report, roc_auc_score, hamming_loss, accuracy_score\n",
        "from keras import optimizers\n",
        "import os\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, Bidirectional\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, LSTM, Conv1D, GlobalMaxPooling1D, CuDNNLSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ld3ptbEqsA",
        "colab_type": "code",
        "outputId": "00f0bb32-cab4-4256-afe0-2f8ea13a3359",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "base_dir = \"gdrive/My Drive/fastai-v3/sdgs/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi63MqzME0Ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT_DATA_DIR = f\"{base_dir}dataset/cleanup_labelled.csv\"\n",
        "CROSS_FOLDS = f\"{base_dir}dataset/cross_validation/\"\n",
        "GLOVE_DIR = f\"{base_dir}embeddings/glove/glove.6B/\"\n",
        "EMBEDDINGS_DIR = f\"{base_dir}embeddings/glove/\"\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 300\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "labels_index = [str(i) for i in range(1,18)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-DVSj0NhHIj",
        "colab_type": "code",
        "outputId": "791bf8f7-43dc-4451-bd41-3830b95b17ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Load pretrained embeddings in an index mapping words in the embeddings set\n",
        "# to their embeddings vector\n",
        "print('Indexing word vectors.')\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt')) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")\n",
        "\n",
        "# second, prepare text samples and their labels\n",
        "print('Processing text dataset')\n",
        "df = pd.read_csv(TEXT_DATA_DIR)\n",
        "df.labels = df.labels.str.split('|').apply(lambda x: [int(i) for i in x])\n",
        "\n",
        "# finally, vectorize the text samples into a 2D integer tensor\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(df.text)\n",
        "sequences = tokenizer.texts_to_sequences(df.text)\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels = mlb.fit_transform(df.labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n",
            "Processing text dataset\n",
            "Found 14736 unique tokens.\n",
            "Shape of data tensor: (5182, 1000)\n",
            "Shape of label tensor: (5182, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLEjjQVeSvVn",
        "colab_type": "code",
        "outputId": "47f1a701-6185-47c3-8a83-5762ee2b6984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "results = []\n",
        "arch = 'Conv1D_glorot_uniform'\n",
        "\n",
        "# Cross-validation: split the data into a training set and a test set\n",
        "for fold in os.listdir(CROSS_FOLDS):\n",
        "    train_index = np.load(f\"{CROSS_FOLDS}{fold}/train.npy\")\n",
        "    val_index = np.load(f\"{CROSS_FOLDS}{fold}/val.npy\")\n",
        "    #test_index = np.load(f\"{CROSS_FOLDS}{fold}/test.npy\")\n",
        "\n",
        "    x_train, x_val = data[train_index], data[val_index]\n",
        "    y_train, y_val = labels[train_index], labels[val_index]\n",
        "    \n",
        "    print(fold)\n",
        "    print('Preparing embedding matrix.')\n",
        "    # prepare embedding matrix\n",
        "    num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        # Ignore word if not in the n most common words\n",
        "        if i > MAX_NUM_WORDS:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    # load pre-trained word embeddings into an Embedding layer\n",
        "    # note that we set trainable = False so as to keep the embeddings fixed\n",
        "    embedding_layer = Embedding(num_words,\n",
        "                                EMBEDDING_DIM,\n",
        "                                embeddings_initializer=Constant(embedding_matrix),\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=False)\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    \n",
        "    \n",
        "    # \n",
        "    if arch == \"Bidirectional_LSTM\":\n",
        "        x = Bidirectional(LSTM(25, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedded_sequences)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dense(50, activation=\"relu\")(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(17, activation=\"sigmoid\")(x)\n",
        "        model = Model(inputs=sequence_input, outputs=x)\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    \n",
        "    # 0.179 with 10 epochs, 300 dimensions\n",
        "    if arch == \"convnet\":\n",
        "        # 1D convnet with global maxpooling\n",
        "        x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "        x = MaxPooling1D(5)(x)\n",
        "        x = Conv1D(128, 5, activation='relu')(x)\n",
        "        x = MaxPooling1D(5)(x)\n",
        "        x = Conv1D(128, 5, activation='relu')(x)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        preds = Dense(len(labels_index), activation='sigmoid')(x)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='binary_crossentropy', \n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy'])\n",
        "        \n",
        "    \n",
        "    # 0.131 with 20 epochs, 300 dimensions\n",
        "    if arch == \"Conv1D_glorot_uniform\":\n",
        "        x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(embedded_sequences)\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "        x = concatenate([avg_pool, max_pool])\n",
        "        preds = Dense(len(labels_index), activation='sigmoid')(x)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='binary_crossentropy', \n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy'])\n",
        "   \n",
        "\n",
        "        \n",
        "    if arch == \"convolution1d\":\n",
        "        #https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py\n",
        "        model = Sequential()\n",
        "\n",
        "        # we start off with an efficient embedding layer which maps\n",
        "        # our vocab indices into embedding_dims dimensions\n",
        "        #model.add(embedded_sequences)\n",
        "        model.add(Embedding(num_words,\n",
        "                    EMBEDDING_DIM,\n",
        "                    input_length=MAX_SEQUENCE_LENGTH))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        # we add a Convolution1D, which will learn filters\n",
        "        # word group filters of size filter_length:\n",
        "        model.add(Conv1D(filters,\n",
        "                         kernel_size,\n",
        "                         padding='valid',\n",
        "                         activation='relu',\n",
        "                         strides=1))\n",
        "        # we use max pooling:\n",
        "        model.add(GlobalMaxPooling1D())\n",
        "\n",
        "        # We add a vanilla hidden layer:\n",
        "        model.add(Dense(hidden_dims))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "        # We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "        model.add(len(labels_index))\n",
        "        model.add(Activation('sigmoid'))\n",
        "\n",
        "\n",
        "    model.fit(x_train, y_train,\n",
        "            batch_size=128,\n",
        "            epochs=NUM_EPOCHS,\n",
        "            validation_data=(x_val, y_val))\n",
        "    \n",
        "    model.save(EMBEDDINGS_DIR + f\"{arch}_{NUM_EPOCHS}epochs_{EMBEDDING_DIM}D_batchsize{BATCH_SIZE}_5fold-cross-val_{fold}.h5\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold_1\n",
            "Preparing embedding matrix.\n",
            "Train on 4173 samples, validate on 516 samples\n",
            "Epoch 1/20\n",
            "4173/4173 [==============================] - 4s 875us/step - loss: 0.3347 - acc: 0.8917 - val_loss: 0.2629 - val_acc: 0.9203\n",
            "Epoch 2/20\n",
            "4173/4173 [==============================] - 1s 358us/step - loss: 0.2445 - acc: 0.9204 - val_loss: 0.2272 - val_acc: 0.9246\n",
            "Epoch 3/20\n",
            "4173/4173 [==============================] - 2s 362us/step - loss: 0.2099 - acc: 0.9279 - val_loss: 0.2023 - val_acc: 0.9335\n",
            "Epoch 4/20\n",
            "4173/4173 [==============================] - 2s 362us/step - loss: 0.1868 - acc: 0.9355 - val_loss: 0.1842 - val_acc: 0.9360\n",
            "Epoch 5/20\n",
            "4173/4173 [==============================] - 1s 359us/step - loss: 0.1705 - acc: 0.9414 - val_loss: 0.1728 - val_acc: 0.9406\n",
            "Epoch 6/20\n",
            "4173/4173 [==============================] - 1s 356us/step - loss: 0.1574 - acc: 0.9456 - val_loss: 0.1667 - val_acc: 0.9412\n",
            "Epoch 7/20\n",
            "4173/4173 [==============================] - 1s 359us/step - loss: 0.1471 - acc: 0.9485 - val_loss: 0.1606 - val_acc: 0.9451\n",
            "Epoch 8/20\n",
            "4173/4173 [==============================] - 1s 355us/step - loss: 0.1380 - acc: 0.9513 - val_loss: 0.1527 - val_acc: 0.9471\n",
            "Epoch 9/20\n",
            "4173/4173 [==============================] - 1s 352us/step - loss: 0.1298 - acc: 0.9540 - val_loss: 0.1495 - val_acc: 0.9474\n",
            "Epoch 10/20\n",
            "4173/4173 [==============================] - 2s 365us/step - loss: 0.1223 - acc: 0.9563 - val_loss: 0.1442 - val_acc: 0.9487\n",
            "Epoch 11/20\n",
            "4173/4173 [==============================] - 2s 372us/step - loss: 0.1158 - acc: 0.9589 - val_loss: 0.1419 - val_acc: 0.9482\n",
            "Epoch 12/20\n",
            "4173/4173 [==============================] - 2s 380us/step - loss: 0.1097 - acc: 0.9608 - val_loss: 0.1383 - val_acc: 0.9504\n",
            "Epoch 13/20\n",
            "4173/4173 [==============================] - 2s 391us/step - loss: 0.1042 - acc: 0.9627 - val_loss: 0.1355 - val_acc: 0.9512\n",
            "Epoch 14/20\n",
            "4173/4173 [==============================] - 2s 394us/step - loss: 0.0988 - acc: 0.9649 - val_loss: 0.1350 - val_acc: 0.9512\n",
            "Epoch 15/20\n",
            "4173/4173 [==============================] - 2s 363us/step - loss: 0.0943 - acc: 0.9668 - val_loss: 0.1316 - val_acc: 0.9525\n",
            "Epoch 16/20\n",
            "4173/4173 [==============================] - 2s 381us/step - loss: 0.0893 - acc: 0.9690 - val_loss: 0.1301 - val_acc: 0.9542\n",
            "Epoch 17/20\n",
            "4173/4173 [==============================] - 2s 389us/step - loss: 0.0852 - acc: 0.9704 - val_loss: 0.1292 - val_acc: 0.9550\n",
            "Epoch 18/20\n",
            "4173/4173 [==============================] - 2s 384us/step - loss: 0.0813 - acc: 0.9723 - val_loss: 0.1276 - val_acc: 0.9549\n",
            "Epoch 19/20\n",
            "4173/4173 [==============================] - 2s 384us/step - loss: 0.0775 - acc: 0.9733 - val_loss: 0.1270 - val_acc: 0.9552\n",
            "Epoch 20/20\n",
            "4173/4173 [==============================] - 2s 366us/step - loss: 0.0736 - acc: 0.9754 - val_loss: 0.1267 - val_acc: 0.9550\n",
            "fold_2\n",
            "Preparing embedding matrix.\n",
            "Train on 4151 samples, validate on 514 samples\n",
            "Epoch 1/20\n",
            "4151/4151 [==============================] - 4s 990us/step - loss: 0.3074 - acc: 0.9049 - val_loss: 0.2537 - val_acc: 0.9205\n",
            "Epoch 2/20\n",
            "4151/4151 [==============================] - 2s 362us/step - loss: 0.2357 - acc: 0.9227 - val_loss: 0.2214 - val_acc: 0.9272\n",
            "Epoch 3/20\n",
            "4151/4151 [==============================] - 2s 380us/step - loss: 0.2055 - acc: 0.9303 - val_loss: 0.2018 - val_acc: 0.9333\n",
            "Epoch 4/20\n",
            "4151/4151 [==============================] - 2s 369us/step - loss: 0.1850 - acc: 0.9366 - val_loss: 0.1928 - val_acc: 0.9361\n",
            "Epoch 5/20\n",
            "4151/4151 [==============================] - 1s 359us/step - loss: 0.1701 - acc: 0.9413 - val_loss: 0.1804 - val_acc: 0.9384\n",
            "Epoch 6/20\n",
            "4151/4151 [==============================] - 1s 357us/step - loss: 0.1578 - acc: 0.9448 - val_loss: 0.1720 - val_acc: 0.9422\n",
            "Epoch 7/20\n",
            "4151/4151 [==============================] - 2s 370us/step - loss: 0.1477 - acc: 0.9486 - val_loss: 0.1643 - val_acc: 0.9440\n",
            "Epoch 8/20\n",
            "4151/4151 [==============================] - 2s 367us/step - loss: 0.1390 - acc: 0.9506 - val_loss: 0.1611 - val_acc: 0.9444\n",
            "Epoch 9/20\n",
            "4151/4151 [==============================] - 2s 368us/step - loss: 0.1315 - acc: 0.9533 - val_loss: 0.1553 - val_acc: 0.9463\n",
            "Epoch 10/20\n",
            "4151/4151 [==============================] - 2s 375us/step - loss: 0.1247 - acc: 0.9554 - val_loss: 0.1538 - val_acc: 0.9458\n",
            "Epoch 11/20\n",
            "4151/4151 [==============================] - 2s 376us/step - loss: 0.1185 - acc: 0.9580 - val_loss: 0.1522 - val_acc: 0.9470\n",
            "Epoch 12/20\n",
            "4151/4151 [==============================] - 1s 359us/step - loss: 0.1130 - acc: 0.9596 - val_loss: 0.1472 - val_acc: 0.9476\n",
            "Epoch 13/20\n",
            "4151/4151 [==============================] - 2s 380us/step - loss: 0.1080 - acc: 0.9613 - val_loss: 0.1460 - val_acc: 0.9494\n",
            "Epoch 14/20\n",
            "4151/4151 [==============================] - 2s 370us/step - loss: 0.1031 - acc: 0.9639 - val_loss: 0.1448 - val_acc: 0.9502\n",
            "Epoch 15/20\n",
            "4151/4151 [==============================] - 2s 391us/step - loss: 0.0986 - acc: 0.9650 - val_loss: 0.1426 - val_acc: 0.9504\n",
            "Epoch 16/20\n",
            "4151/4151 [==============================] - 2s 392us/step - loss: 0.0944 - acc: 0.9665 - val_loss: 0.1418 - val_acc: 0.9502\n",
            "Epoch 17/20\n",
            "4151/4151 [==============================] - 2s 374us/step - loss: 0.0902 - acc: 0.9685 - val_loss: 0.1387 - val_acc: 0.9515\n",
            "Epoch 18/20\n",
            "4151/4151 [==============================] - 2s 400us/step - loss: 0.0867 - acc: 0.9696 - val_loss: 0.1401 - val_acc: 0.9509\n",
            "Epoch 19/20\n",
            "4151/4151 [==============================] - 2s 390us/step - loss: 0.0830 - acc: 0.9718 - val_loss: 0.1378 - val_acc: 0.9516\n",
            "Epoch 20/20\n",
            "4151/4151 [==============================] - 2s 379us/step - loss: 0.0793 - acc: 0.9731 - val_loss: 0.1390 - val_acc: 0.9532\n",
            "fold_3\n",
            "Preparing embedding matrix.\n",
            "Train on 4142 samples, validate on 513 samples\n",
            "Epoch 1/20\n",
            "4142/4142 [==============================] - 5s 1ms/step - loss: 0.3365 - acc: 0.8874 - val_loss: 0.2477 - val_acc: 0.9265\n",
            "Epoch 2/20\n",
            "4142/4142 [==============================] - 2s 376us/step - loss: 0.2459 - acc: 0.9199 - val_loss: 0.2092 - val_acc: 0.9314\n",
            "Epoch 3/20\n",
            "4142/4142 [==============================] - 1s 361us/step - loss: 0.2122 - acc: 0.9264 - val_loss: 0.1862 - val_acc: 0.9396\n",
            "Epoch 4/20\n",
            "4142/4142 [==============================] - 2s 380us/step - loss: 0.1872 - acc: 0.9336 - val_loss: 0.1707 - val_acc: 0.9450\n",
            "Epoch 5/20\n",
            "4142/4142 [==============================] - 2s 401us/step - loss: 0.1697 - acc: 0.9392 - val_loss: 0.1589 - val_acc: 0.9481\n",
            "Epoch 6/20\n",
            "4142/4142 [==============================] - 2s 389us/step - loss: 0.1561 - acc: 0.9446 - val_loss: 0.1508 - val_acc: 0.9509\n",
            "Epoch 7/20\n",
            "4142/4142 [==============================] - 2s 381us/step - loss: 0.1454 - acc: 0.9477 - val_loss: 0.1505 - val_acc: 0.9520\n",
            "Epoch 8/20\n",
            "4142/4142 [==============================] - 2s 373us/step - loss: 0.1366 - acc: 0.9517 - val_loss: 0.1413 - val_acc: 0.9540\n",
            "Epoch 9/20\n",
            "4142/4142 [==============================] - 2s 364us/step - loss: 0.1291 - acc: 0.9543 - val_loss: 0.1368 - val_acc: 0.9552\n",
            "Epoch 10/20\n",
            "4142/4142 [==============================] - 1s 362us/step - loss: 0.1223 - acc: 0.9563 - val_loss: 0.1373 - val_acc: 0.9551\n",
            "Epoch 11/20\n",
            "4142/4142 [==============================] - 2s 364us/step - loss: 0.1161 - acc: 0.9585 - val_loss: 0.1310 - val_acc: 0.9564\n",
            "Epoch 12/20\n",
            "4142/4142 [==============================] - 1s 358us/step - loss: 0.1106 - acc: 0.9608 - val_loss: 0.1293 - val_acc: 0.9569\n",
            "Epoch 13/20\n",
            "4142/4142 [==============================] - 1s 362us/step - loss: 0.1052 - acc: 0.9623 - val_loss: 0.1303 - val_acc: 0.9562\n",
            "Epoch 14/20\n",
            "4142/4142 [==============================] - 2s 369us/step - loss: 0.1007 - acc: 0.9646 - val_loss: 0.1247 - val_acc: 0.9585\n",
            "Epoch 15/20\n",
            "4142/4142 [==============================] - 1s 348us/step - loss: 0.0959 - acc: 0.9664 - val_loss: 0.1247 - val_acc: 0.9587\n",
            "Epoch 16/20\n",
            "4142/4142 [==============================] - 1s 358us/step - loss: 0.0918 - acc: 0.9681 - val_loss: 0.1232 - val_acc: 0.9581\n",
            "Epoch 17/20\n",
            "4142/4142 [==============================] - 2s 376us/step - loss: 0.0875 - acc: 0.9698 - val_loss: 0.1226 - val_acc: 0.9578\n",
            "Epoch 18/20\n",
            "4142/4142 [==============================] - 2s 369us/step - loss: 0.0841 - acc: 0.9710 - val_loss: 0.1210 - val_acc: 0.9584\n",
            "Epoch 19/20\n",
            "4142/4142 [==============================] - 2s 380us/step - loss: 0.0803 - acc: 0.9723 - val_loss: 0.1219 - val_acc: 0.9594\n",
            "Epoch 20/20\n",
            "4142/4142 [==============================] - 2s 371us/step - loss: 0.0768 - acc: 0.9741 - val_loss: 0.1233 - val_acc: 0.9587\n",
            "fold_4\n",
            "Preparing embedding matrix.\n",
            "Train on 4140 samples, validate on 512 samples\n",
            "Epoch 1/20\n",
            "4140/4140 [==============================] - 4s 1ms/step - loss: 0.3205 - acc: 0.9001 - val_loss: 0.2502 - val_acc: 0.9231\n",
            "Epoch 2/20\n",
            "4140/4140 [==============================] - 2s 365us/step - loss: 0.2397 - acc: 0.9204 - val_loss: 0.2164 - val_acc: 0.9292\n",
            "Epoch 3/20\n",
            "4140/4140 [==============================] - 2s 393us/step - loss: 0.2076 - acc: 0.9282 - val_loss: 0.1953 - val_acc: 0.9343\n",
            "Epoch 4/20\n",
            "4140/4140 [==============================] - 2s 374us/step - loss: 0.1858 - acc: 0.9350 - val_loss: 0.1824 - val_acc: 0.9414\n",
            "Epoch 5/20\n",
            "4140/4140 [==============================] - 2s 386us/step - loss: 0.1694 - acc: 0.9400 - val_loss: 0.1705 - val_acc: 0.9419\n",
            "Epoch 6/20\n",
            "4140/4140 [==============================] - 2s 384us/step - loss: 0.1568 - acc: 0.9440 - val_loss: 0.1618 - val_acc: 0.9461\n",
            "Epoch 7/20\n",
            "4140/4140 [==============================] - 2s 383us/step - loss: 0.1463 - acc: 0.9481 - val_loss: 0.1580 - val_acc: 0.9477\n",
            "Epoch 8/20\n",
            "4140/4140 [==============================] - 2s 378us/step - loss: 0.1376 - acc: 0.9510 - val_loss: 0.1530 - val_acc: 0.9503\n",
            "Epoch 9/20\n",
            "4140/4140 [==============================] - 2s 392us/step - loss: 0.1300 - acc: 0.9535 - val_loss: 0.1479 - val_acc: 0.9523\n",
            "Epoch 10/20\n",
            "4140/4140 [==============================] - 2s 404us/step - loss: 0.1227 - acc: 0.9562 - val_loss: 0.1540 - val_acc: 0.9505\n",
            "Epoch 11/20\n",
            "4140/4140 [==============================] - 2s 380us/step - loss: 0.1173 - acc: 0.9585 - val_loss: 0.1414 - val_acc: 0.9530\n",
            "Epoch 12/20\n",
            "4140/4140 [==============================] - 2s 390us/step - loss: 0.1112 - acc: 0.9600 - val_loss: 0.1379 - val_acc: 0.9543\n",
            "Epoch 13/20\n",
            "4140/4140 [==============================] - 2s 381us/step - loss: 0.1063 - acc: 0.9623 - val_loss: 0.1360 - val_acc: 0.9539\n",
            "Epoch 14/20\n",
            "4140/4140 [==============================] - 2s 362us/step - loss: 0.1012 - acc: 0.9637 - val_loss: 0.1344 - val_acc: 0.9538\n",
            "Epoch 15/20\n",
            "4140/4140 [==============================] - 2s 375us/step - loss: 0.0969 - acc: 0.9653 - val_loss: 0.1337 - val_acc: 0.9547\n",
            "Epoch 16/20\n",
            "4140/4140 [==============================] - 2s 384us/step - loss: 0.0923 - acc: 0.9673 - val_loss: 0.1303 - val_acc: 0.9558\n",
            "Epoch 17/20\n",
            "4140/4140 [==============================] - 1s 358us/step - loss: 0.0883 - acc: 0.9690 - val_loss: 0.1279 - val_acc: 0.9569\n",
            "Epoch 18/20\n",
            "4140/4140 [==============================] - 2s 371us/step - loss: 0.0846 - acc: 0.9705 - val_loss: 0.1268 - val_acc: 0.9568\n",
            "Epoch 19/20\n",
            "4140/4140 [==============================] - 1s 355us/step - loss: 0.0808 - acc: 0.9717 - val_loss: 0.1262 - val_acc: 0.9573\n",
            "Epoch 20/20\n",
            "4140/4140 [==============================] - 1s 357us/step - loss: 0.0773 - acc: 0.9732 - val_loss: 0.1272 - val_acc: 0.9565\n",
            "fold_5\n",
            "Preparing embedding matrix.\n",
            "Train on 4155 samples, validate on 514 samples\n",
            "Epoch 1/20\n",
            "4155/4155 [==============================] - 5s 1ms/step - loss: 0.3365 - acc: 0.8930 - val_loss: 0.2672 - val_acc: 0.9179\n",
            "Epoch 2/20\n",
            "4155/4155 [==============================] - 1s 360us/step - loss: 0.2459 - acc: 0.9203 - val_loss: 0.2316 - val_acc: 0.9241\n",
            "Epoch 3/20\n",
            "4155/4155 [==============================] - 2s 373us/step - loss: 0.2114 - acc: 0.9274 - val_loss: 0.2085 - val_acc: 0.9290\n",
            "Epoch 4/20\n",
            "4155/4155 [==============================] - 2s 366us/step - loss: 0.1875 - acc: 0.9353 - val_loss: 0.1920 - val_acc: 0.9353\n",
            "Epoch 5/20\n",
            "4155/4155 [==============================] - 2s 373us/step - loss: 0.1703 - acc: 0.9408 - val_loss: 0.1810 - val_acc: 0.9390\n",
            "Epoch 6/20\n",
            "4155/4155 [==============================] - 2s 368us/step - loss: 0.1572 - acc: 0.9452 - val_loss: 0.1737 - val_acc: 0.9423\n",
            "Epoch 7/20\n",
            "4155/4155 [==============================] - 2s 364us/step - loss: 0.1463 - acc: 0.9489 - val_loss: 0.1699 - val_acc: 0.9460\n",
            "Epoch 8/20\n",
            "4155/4155 [==============================] - 2s 362us/step - loss: 0.1372 - acc: 0.9518 - val_loss: 0.1673 - val_acc: 0.9459\n",
            "Epoch 9/20\n",
            "4155/4155 [==============================] - 2s 374us/step - loss: 0.1293 - acc: 0.9542 - val_loss: 0.1586 - val_acc: 0.9470\n",
            "Epoch 10/20\n",
            "4155/4155 [==============================] - 2s 375us/step - loss: 0.1224 - acc: 0.9566 - val_loss: 0.1571 - val_acc: 0.9493\n",
            "Epoch 11/20\n",
            "4155/4155 [==============================] - 2s 373us/step - loss: 0.1162 - acc: 0.9587 - val_loss: 0.1538 - val_acc: 0.9499\n",
            "Epoch 12/20\n",
            "4155/4155 [==============================] - 2s 370us/step - loss: 0.1107 - acc: 0.9607 - val_loss: 0.1519 - val_acc: 0.9501\n",
            "Epoch 13/20\n",
            "4155/4155 [==============================] - 2s 368us/step - loss: 0.1052 - acc: 0.9626 - val_loss: 0.1490 - val_acc: 0.9515\n",
            "Epoch 14/20\n",
            "4155/4155 [==============================] - 2s 375us/step - loss: 0.1004 - acc: 0.9643 - val_loss: 0.1475 - val_acc: 0.9523\n",
            "Epoch 15/20\n",
            "4155/4155 [==============================] - 2s 365us/step - loss: 0.0956 - acc: 0.9666 - val_loss: 0.1473 - val_acc: 0.9516\n",
            "Epoch 16/20\n",
            "4155/4155 [==============================] - 2s 367us/step - loss: 0.0917 - acc: 0.9676 - val_loss: 0.1457 - val_acc: 0.9535\n",
            "Epoch 17/20\n",
            "4155/4155 [==============================] - 2s 365us/step - loss: 0.0872 - acc: 0.9696 - val_loss: 0.1449 - val_acc: 0.9538\n",
            "Epoch 18/20\n",
            "4155/4155 [==============================] - 2s 373us/step - loss: 0.0839 - acc: 0.9708 - val_loss: 0.1439 - val_acc: 0.9534\n",
            "Epoch 19/20\n",
            "4155/4155 [==============================] - 2s 373us/step - loss: 0.0802 - acc: 0.9723 - val_loss: 0.1425 - val_acc: 0.9537\n",
            "Epoch 20/20\n",
            "4155/4155 [==============================] - 2s 363us/step - loss: 0.0769 - acc: 0.9743 - val_loss: 0.1446 - val_acc: 0.9531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dnhmTNH7KeM",
        "colab_type": "text"
      },
      "source": [
        "# Load and evaluate folds on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip3GMV8X03J_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_avg(models_testx_testy, labels_, thres=0.3):\n",
        "    def calc(model, test_x, test_y):\n",
        "        predictions = model.predict(test_x)>thres\n",
        "        metrics = classification_report(test_y, predictions, target_names=labels_, output_dict=True)\n",
        "        metrics_df = pd.DataFrame.from_dict(metrics)\n",
        "        h = hamming_loss(test_y, predictions)\n",
        "        roc = roc_auc_score(test_y, predictions, average='micro')\n",
        "        return metrics_df, h, roc\n",
        "\n",
        "    model_1, test_x_first, test_y_first = models_testx_testy[0]\n",
        "    metrics_agg, ham, roc = calc(model_1, test_x_first, test_y_first)\n",
        "    n = len(models_testx_testy)\n",
        "\n",
        "    for model, test_x, test_y in models_testx_testy[1:]:\n",
        "        metrics, h, r = calc(model, test_x, test_y)\n",
        "        metrics_agg += metrics\n",
        "        ham += h\n",
        "        roc += r\n",
        "\n",
        "    return metrics_agg/n, ham/n, roc/n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVBmo_HUH7O5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a3c90dc2-8e3b-445c-fd36-60d0d058d33d"
      },
      "source": [
        "loaded_arch = 'Conv1D_glorot_uniform'\n",
        "loaded_models = []\n",
        "for fold in os.listdir(CROSS_FOLDS):\n",
        "    print(f\"Loading {fold}...\")\n",
        "    test_index = np.load(f\"{CROSS_FOLDS}{fold}/test.npy\")\n",
        "\n",
        "    x_test = data[test_index]\n",
        "    y_test = labels[test_index]\n",
        "    \n",
        "    load_dir = EMBEDDINGS_DIR + f\"{loaded_arch}_{NUM_EPOCHS}epochs_{EMBEDDING_DIM}D_batchsize{BATCH_SIZE}_5fold-cross-val_{fold}.h5\"\n",
        "    loaded_model = load_model(load_dir)\n",
        "    \n",
        "    loaded_models.append((loaded_model, x_test, y_test))\n",
        "print(f\"Finished loading the {loaded_arch} models.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading fold_1...\n",
            "Loading fold_2...\n",
            "Loading fold_3...\n",
            "Loading fold_4...\n",
            "Loading fold_5...\n",
            "Finished loading the Conv1D_glorot_uniform models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYVLRfpK7QiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_results = metrics_avg(loaded_models, labels_index, thres=.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLr3rO4g8mDL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6239f962-c008-41a4-a2d4-a4594c02e1bf"
      },
      "source": [
        "avg_results[2]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8148906161025143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}