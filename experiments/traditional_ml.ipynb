{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "traditional_ml.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vondersam/sdgs_text_classifier/blob/master/experiments/traditional_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6p8vLIwha9i",
        "colab_type": "code",
        "outputId": "a94edd48-4995-415e-dc06-1253155788d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbXFaXGGhr21",
        "colab_type": "code",
        "outputId": "056f12e4-5e38-4ee4-d3db-d865264e4a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "base_dir = \"gdrive/My Drive/fastai-v3/sdgs/\"\n",
        "labelled_dataset = base_dir + \"dataset/cleanup_labelled.csv\"\n",
        "CROSS_FOLDS = f\"{base_dir}dataset/cross_validation/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEVes-M5h8r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labelled = pd.read_csv(labelled_dataset)\n",
        "labelled.labels = labelled.labels.str.split('|').apply(lambda x: [int(i) for i in x])\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "data_x = labelled[['text']].values\n",
        "x = np.array([x[0] for x in data_x.tolist()])\n",
        "y = mlb.fit_transform(labelled.labels)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "labels = [str(i) for i in range(1,18)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBklIRcbhscE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splits = []\n",
        "for fold in os.listdir(CROSS_FOLDS):\n",
        "    train_index = np.load(f\"{CROSS_FOLDS}{fold}/train.npy\")\n",
        "    val_index = np.load(f\"{CROSS_FOLDS}{fold}/val.npy\")\n",
        "    splits.append((train_index, val_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUf7LEcxiS31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grid_search(x, y, parameters, pipeline, splits):\n",
        "    '''Train pipeline, test and print results'''\n",
        "    gs = GridSearchCV(pipeline, \n",
        "                      parameters, \n",
        "                      cv=splits, \n",
        "                      n_jobs=5, \n",
        "                      verbose=10, \n",
        "                      return_train_score=True, \n",
        "                      scoring='f1_micro')\n",
        "    gs.fit(x, y)\n",
        "    print()\n",
        "    print(\"Best parameters set:\")\n",
        "    print(gs.best_estimator_.steps)\n",
        "    print()\n",
        "    results = gs.cv_results_\n",
        "    print(f\"Mean train scores: {results['mean_train_score']}\")\n",
        "    print(f\"Mean validation scores: {results['mean_test_score']}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfjWBDTr-NdK",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXmcCcXm9y8f",
        "colab_type": "code",
        "outputId": "7ed5acb2-daee-463d-89d5-c6270147fefd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
        "                    fit_prior=True, class_prior=None))),\n",
        "            ])\n",
        "parameters = {\n",
        "                'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "                'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "                'clf__estimator__alpha': (1e-2, 1e-3)\n",
        "            }\n",
        "grid_search(x, y, parameters, pipeline, splits)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:    7.7s\n",
            "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:   16.5s\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:   32.6s\n",
            "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:   44.8s\n",
            "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=5)]: Done  51 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=5)]: Done  62 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=5)]: Done  75 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=5)]: Done  90 out of  90 | elapsed:  2.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Best parameters set:\n",
            "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.25, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
            "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
            "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
            "                            'been', 'before', 'being', 'below', 'between',\n",
            "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)), ('clf', OneVsRestClassifier(estimator=MultinomialNB(alpha=0.01, class_prior=None,\n",
            "                                            fit_prior=True),\n",
            "                    n_jobs=None))]\n",
            "\n",
            "Mean train score: [0.88382074 0.98138123 0.97579812 0.88124053 0.98106887 0.97567946\n",
            " 0.88124053 0.98106887 0.97567946 0.90170611 0.98557881 0.98512222\n",
            " 0.89956406 0.98557758 0.98505287 0.89956406 0.98557758 0.98505287]\n",
            "Mean validation score: [0.54239689 0.65258213 0.66209054 0.54033704 0.65196753 0.65998952\n",
            " 0.54033704 0.65196753 0.65998952 0.51788825 0.63263591 0.65253621\n",
            " 0.51603845 0.63320208 0.65183251 0.51603845 0.63320208 0.65183251]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6np7GSQXErBx",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pMWIkVZEf2m",
        "colab_type": "code",
        "outputId": "48bf17b0-fdab-44ec-86d4-e742a77b68d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "    ('clf', OneVsRestClassifier(LinearSVC())),\n",
        "])\n",
        "parameters = {\n",
        "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    \"clf__estimator__C\": [0.01, 0.1, 1],\n",
        "    \"clf__estimator__class_weight\": ['balanced', None],\n",
        "}\n",
        "grid_search(x, y, parameters, pipeline, splits)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:    3.8s\n",
            "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:   13.6s\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:   31.6s\n",
            "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:   43.4s\n",
            "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=5)]: Done  51 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=5)]: Done  62 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=5)]: Done  75 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=5)]: Done  88 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=5)]: Done 103 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=5)]: Done 118 tasks      | elapsed:  4.3min\n",
            "[Parallel(n_jobs=5)]: Done 135 tasks      | elapsed:  5.0min\n",
            "[Parallel(n_jobs=5)]: Done 152 tasks      | elapsed:  5.5min\n",
            "[Parallel(n_jobs=5)]: Done 171 tasks      | elapsed:  6.2min\n",
            "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:  7.2min\n",
            "[Parallel(n_jobs=5)]: Done 211 tasks      | elapsed:  9.4min\n",
            "[Parallel(n_jobs=5)]: Done 232 tasks      | elapsed: 11.0min\n",
            "[Parallel(n_jobs=5)]: Done 255 tasks      | elapsed: 12.1min\n",
            "[Parallel(n_jobs=5)]: Done 270 out of 270 | elapsed: 12.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Best parameters set:\n",
            "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
            "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
            "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
            "                            'been', 'before', 'being', 'below', 'between',\n",
            "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LinearSVC(C=1, class_weight='balanced', dual=True,\n",
            "                                        fit_intercept=True, intercept_scaling=1,\n",
            "                                        loss='squared_hinge', max_iter=1000,\n",
            "                                        multi_class='ovr', penalty='l2',\n",
            "                                        random_state=None, tol=0.0001,\n",
            "                                        verbose=0),\n",
            "                    n_jobs=None))]\n",
            "\n",
            "Mean train scores: [0.77294833 0.85204538 0.89323386 0.77030056 0.84885368 0.89099056\n",
            " 0.77030056 0.84885368 0.89099056 0.07203024 0.00618181 0.00309553\n",
            " 0.06303203 0.00570288 0.00309553 0.06303203 0.00570288 0.00309553\n",
            " 0.88884474 0.95874936 0.97233492 0.88732735 0.95805672 0.97177693\n",
            " 0.88732735 0.95805672 0.97177693 0.57575521 0.48758409 0.39508222\n",
            " 0.57483189 0.48729448 0.39538995 0.57483189 0.48729448 0.39538995\n",
            " 0.96398181 0.98785065 0.98840078 0.96328752 0.98745277 0.9882041\n",
            " 0.96328752 0.98745277 0.9882041  0.97136943 0.99406589 0.99534927\n",
            " 0.96973065 0.99390942 0.99527989 0.96973065 0.99390942 0.99527989]\n",
            "Mean validation scores: [0.69123257 0.72278014 0.72724794 0.68812894 0.71628516 0.72197627\n",
            " 0.68812894 0.71628516 0.72197627 0.06990639 0.00645839 0.00412055\n",
            " 0.0564234  0.00645839 0.00350896 0.0564234  0.00645839 0.00350896\n",
            " 0.71706419 0.75515093 0.75660924 0.71644835 0.75365843 0.75331609\n",
            " 0.71644835 0.75365843 0.75331609 0.5246548  0.47820268 0.4310089\n",
            " 0.52357481 0.4796591  0.43348646 0.52357481 0.4796591  0.43348646\n",
            " 0.71537598 0.76219688 0.76223206 0.7142166  0.76373858 0.76172959\n",
            " 0.7142166  0.76373858 0.76172959 0.69841614 0.72884177 0.73114328\n",
            " 0.70037614 0.72852174 0.73070509 0.70037614 0.72866054 0.73070509]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlXiPkDuEytV",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_Lao8-DEnN_",
        "colab_type": "code",
        "outputId": "e590a8f4-e6ac-42fb-e380-4cc02ff7cc9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "    ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'))),\n",
        "])\n",
        "parameters = {\n",
        "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    \"clf__estimator__C\": [0.01, 0.1, 1],\n",
        "    \"clf__estimator__class_weight\": ['balanced', None],\n",
        "    \"clf__estimator__multi_class\": ['ovr', 'multinomial']\n",
        "}\n",
        "grid_search(x, y, parameters, pipeline, splits)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:    8.6s\n",
            "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:   38.3s\n",
            "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:  1.6min\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=5)]: Done  51 tasks      | elapsed:  5.4min\n",
            "[Parallel(n_jobs=5)]: Done  62 tasks      | elapsed:  6.9min\n",
            "[Parallel(n_jobs=5)]: Done  75 tasks      | elapsed:  8.9min\n",
            "[Parallel(n_jobs=5)]: Done  88 tasks      | elapsed: 10.9min\n",
            "[Parallel(n_jobs=5)]: Done 103 tasks      | elapsed: 12.4min\n",
            "[Parallel(n_jobs=5)]: Done 118 tasks      | elapsed: 14.0min\n",
            "[Parallel(n_jobs=5)]: Done 135 tasks      | elapsed: 15.6min\n",
            "[Parallel(n_jobs=5)]: Done 152 tasks      | elapsed: 17.6min\n",
            "[Parallel(n_jobs=5)]: Done 171 tasks      | elapsed: 20.3min\n",
            "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed: 22.0min\n",
            "[Parallel(n_jobs=5)]: Done 211 tasks      | elapsed: 23.5min\n",
            "[Parallel(n_jobs=5)]: Done 232 tasks      | elapsed: 24.8min\n",
            "[Parallel(n_jobs=5)]: Done 255 tasks      | elapsed: 26.9min\n",
            "[Parallel(n_jobs=5)]: Done 278 tasks      | elapsed: 28.6min\n",
            "[Parallel(n_jobs=5)]: Done 303 tasks      | elapsed: 30.0min\n",
            "[Parallel(n_jobs=5)]: Done 328 tasks      | elapsed: 31.7min\n",
            "[Parallel(n_jobs=5)]: Done 355 tasks      | elapsed: 33.3min\n",
            "[Parallel(n_jobs=5)]: Done 382 tasks      | elapsed: 37.6min\n",
            "[Parallel(n_jobs=5)]: Done 411 tasks      | elapsed: 45.4min\n",
            "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed: 57.9min\n",
            "[Parallel(n_jobs=5)]: Done 471 tasks      | elapsed: 66.3min\n",
            "[Parallel(n_jobs=5)]: Done 502 tasks      | elapsed: 69.1min\n",
            "[Parallel(n_jobs=5)]: Done 540 out of 540 | elapsed: 73.9min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Best parameters set:\n",
            "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.25, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
            "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
            "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
            "                            'been', 'before', 'being', 'below', 'between',\n",
            "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LogisticRegression(C=1, class_weight='balanced',\n",
            "                                                 dual=False, fit_intercept=True,\n",
            "                                                 intercept_scaling=1,\n",
            "                                                 l1_ratio=None, max_iter=100,\n",
            "                                                 multi_class='ovr', n_jobs=None,\n",
            "                                                 penalty='l2',\n",
            "                                                 random_state=None,\n",
            "                                                 solver='sag', tol=0.0001,\n",
            "                                                 verbose=0, warm_start=False),\n",
            "                    n_jobs=None))]\n",
            "\n",
            "Mean train scores: [0.71173682 0.77697963 0.81386591 0.70391098 0.77194347 0.81022469\n",
            " 0.70411356 0.77200972 0.80994233 0.72460563 0.79198746 0.82880731\n",
            " 0.71845348 0.78788701 0.825587   0.71858288 0.78798999 0.8256514\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.77576207 0.86146837 0.90319218 0.77304261 0.85816527 0.901181\n",
            " 0.77300832 0.85818952 0.90111916 0.80818784 0.90414044 0.94077658\n",
            " 0.80679987 0.90053967 0.93789457 0.80678715 0.90058401 0.93792812\n",
            " 0.05460194 0.00405803 0.00254609 0.04589755 0.00385192 0.00268333\n",
            " 0.04589755 0.00385192 0.00268333 0.18710012 0.0753853  0.03891922\n",
            " 0.18578649 0.07582503 0.03878756 0.18578649 0.07582503 0.03878756\n",
            " 0.88344878 0.95519249 0.96973288 0.88289275 0.95449698 0.96956937\n",
            " 0.88232043 0.95467973 0.96950898 0.83863305 0.94319369 0.95763089\n",
            " 0.83529162 0.9401308  0.95490003 0.83435054 0.9373421  0.95732457\n",
            " 0.53842762 0.42436727 0.33370738 0.53770059 0.42505249 0.33375524\n",
            " 0.53766435 0.42500999 0.33375524 0.67019333 0.62387455 0.55917563\n",
            " 0.67173422 0.62479204 0.56007461 0.67179429 0.62492126 0.56007484]\n",
            "Mean validation scores: [0.67564074 0.7059686  0.71051405 0.66844108 0.69976085 0.70607837\n",
            " 0.66854569 0.70033487 0.70609599 0.67786271 0.70882909 0.71378035\n",
            " 0.67411611 0.70257507 0.70987338 0.6743223  0.70231985 0.70992551\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.69390518 0.72545524 0.72908992 0.6912414  0.72201224 0.72603783\n",
            " 0.6912414  0.72201224 0.7259348  0.70055958 0.73332417 0.73577836\n",
            " 0.69745261 0.73270567 0.73559387 0.69754992 0.73270567 0.73559387\n",
            " 0.04813402 0.00585286 0.00289548 0.0393736  0.00585286 0.00289548\n",
            " 0.0393736  0.00585286 0.00289548 0.1916123  0.08721381 0.04272109\n",
            " 0.18680926 0.0881392  0.04384319 0.18680926 0.0881392  0.04384319\n",
            " 0.71585408 0.75441297 0.75458996 0.71456135 0.75280116 0.75325123\n",
            " 0.71517983 0.75304333 0.75324417 0.67255892 0.74003016 0.73756923\n",
            " 0.66421345 0.7295356  0.73545831 0.66498539 0.73062041 0.7375956\n",
            " 0.50029206 0.43935878 0.38902827 0.49851857 0.43939077 0.39161177\n",
            " 0.49851857 0.43939077 0.39161177 0.57913382 0.56414074 0.53900445\n",
            " 0.58190362 0.56419852 0.54045572 0.58190362 0.56419852 0.54045572]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvVCq_YGFH8R",
        "colab_type": "text"
      },
      "source": [
        "# Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7CSwel9FJfd",
        "colab_type": "code",
        "outputId": "4636048e-79bf-4ec9-dc2e-a0744b363b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "    ('clf', DecisionTreeClassifier()),\n",
        "])\n",
        "parameters = {\n",
        "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]\n",
        "}\n",
        "grid_search(x, y, parameters, pipeline, splits)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:   19.6s\n",
            "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:  4.6min\n",
            "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:  6.9min\n",
            "[Parallel(n_jobs=5)]: Done  41 out of  45 | elapsed:  9.9min remaining:   58.0s\n",
            "[Parallel(n_jobs=5)]: Done  45 out of  45 | elapsed: 10.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Best parameters set:\n",
            "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.25, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
            "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
            "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
            "                            'been', 'before', 'being', 'below', 'between',\n",
            "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
            "                       max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort=False,\n",
            "                       random_state=None, splitter='best'))]\n",
            "\n",
            "Mean train scores: [0.99723591 0.99770228 0.99770228 0.99756458 0.99784069 0.99784069\n",
            " 0.99756458 0.99784069 0.99784069]\n",
            "Mean validation scores: [0.61193436 0.61485835 0.60765938 0.60602461 0.6039503  0.60307723\n",
            " 0.60399592 0.60201962 0.6012122 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqw-DqlqIV8C",
        "colab_type": "text"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE_pyE6ZF0jM",
        "colab_type": "code",
        "outputId": "1fe76d87-224e-43ea-83f0-086f6e829e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "    ('clf', KNeighborsClassifier()),\n",
        "])\n",
        "parameters = {\n",
        "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    'clf__n_neighbors': (2,3,4,5),\n",
        "    'clf__weights': ('uniform', 'distance'),\n",
        "    'clf__metric': ['minkowski'],\n",
        "    'clf__algorithm': ('ball_tree', 'kd_tree', 'brute')\n",
        "}\n",
        "grid_search(x, y, parameters, pipeline, splits)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:   23.9s\n",
            "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:   59.0s\n",
            "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:  3.3min\n",
            "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  4.0min\n",
            "[Parallel(n_jobs=5)]: Done  51 tasks      | elapsed:  4.8min\n",
            "[Parallel(n_jobs=5)]: Done  62 tasks      | elapsed:  5.3min\n",
            "[Parallel(n_jobs=5)]: Done  75 tasks      | elapsed:  6.0min\n",
            "[Parallel(n_jobs=5)]: Done  88 tasks      | elapsed:  6.7min\n",
            "[Parallel(n_jobs=5)]: Done 103 tasks      | elapsed:  8.1min\n",
            "[Parallel(n_jobs=5)]: Done 118 tasks      | elapsed:  9.5min\n",
            "[Parallel(n_jobs=5)]: Done 135 tasks      | elapsed: 11.0min\n",
            "[Parallel(n_jobs=5)]: Done 152 tasks      | elapsed: 11.8min\n",
            "[Parallel(n_jobs=5)]: Done 171 tasks      | elapsed: 12.7min\n",
            "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed: 14.1min\n",
            "[Parallel(n_jobs=5)]: Done 211 tasks      | elapsed: 16.4min\n",
            "[Parallel(n_jobs=5)]: Done 232 tasks      | elapsed: 18.0min\n",
            "[Parallel(n_jobs=5)]: Done 255 tasks      | elapsed: 19.1min\n",
            "[Parallel(n_jobs=5)]: Done 278 tasks      | elapsed: 20.7min\n",
            "[Parallel(n_jobs=5)]: Done 303 tasks      | elapsed: 23.2min\n",
            "[Parallel(n_jobs=5)]: Done 328 tasks      | elapsed: 24.9min\n",
            "[Parallel(n_jobs=5)]: Done 355 tasks      | elapsed: 25.8min\n",
            "[Parallel(n_jobs=5)]: Done 382 tasks      | elapsed: 27.7min\n",
            "[Parallel(n_jobs=5)]: Done 411 tasks      | elapsed: 29.6min\n",
            "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed: 30.8min\n",
            "[Parallel(n_jobs=5)]: Done 471 tasks      | elapsed: 33.2min\n",
            "[Parallel(n_jobs=5)]: Done 502 tasks      | elapsed: 35.2min\n",
            "[Parallel(n_jobs=5)]: Done 535 tasks      | elapsed: 36.7min\n",
            "[Parallel(n_jobs=5)]: Done 568 tasks      | elapsed: 39.8min\n",
            "[Parallel(n_jobs=5)]: Done 603 tasks      | elapsed: 41.7min\n",
            "[Parallel(n_jobs=5)]: Done 638 tasks      | elapsed: 43.6min\n",
            "[Parallel(n_jobs=5)]: Done 675 tasks      | elapsed: 46.9min\n",
            "[Parallel(n_jobs=5)]: Done 712 tasks      | elapsed: 48.4min\n",
            "[Parallel(n_jobs=5)]: Done 751 tasks      | elapsed: 51.6min\n",
            "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed: 53.9min\n",
            "[Parallel(n_jobs=5)]: Done 831 tasks      | elapsed: 56.8min\n",
            "[Parallel(n_jobs=5)]: Done 872 tasks      | elapsed: 58.9min\n",
            "[Parallel(n_jobs=5)]: Done 915 tasks      | elapsed: 60.9min\n",
            "[Parallel(n_jobs=5)]: Done 958 tasks      | elapsed: 63.5min\n",
            "[Parallel(n_jobs=5)]: Done 1003 tasks      | elapsed: 65.9min\n",
            "[Parallel(n_jobs=5)]: Done 1048 tasks      | elapsed: 69.4min\n",
            "[Parallel(n_jobs=5)]: Done 1080 out of 1080 | elapsed: 71.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Best parameters set:\n",
            "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
            "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
            "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
            "                            'been', 'before', 'being', 'below', 'between',\n",
            "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)), ('clf', KNeighborsClassifier(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
            "                     weights='distance'))]\n",
            "\n",
            "Mean train scores: [0.49281024 0.40116321 0.38039959 0.58334489 0.5275787  0.51279092\n",
            " 0.58334489 0.5275787  0.51279092 0.99723591 0.99770228 0.99770228\n",
            " 0.99756458 0.99784069 0.99784069 0.99756458 0.99784069 0.99784069\n",
            " 0.55530321 0.47592492 0.4580474  0.76823683 0.7784613  0.77446107\n",
            " 0.76823683 0.7784613  0.77446107 0.99723591 0.99770228 0.99770228\n",
            " 0.99756458 0.99784069 0.99784069 0.99756458 0.99784069 0.99784069\n",
            " 0.30843316 0.22113406 0.20106167 0.63842024 0.65954501 0.65764994\n",
            " 0.63842024 0.65954501 0.65764994 0.99723591 0.99770228 0.99770228\n",
            " 0.99756458 0.99784069 0.99784069 0.99756458 0.99784069 0.99784069\n",
            " 0.32362291 0.29484388 0.27835    0.68953662 0.71320175 0.71269051\n",
            " 0.68953662 0.71320175 0.71269051 0.99723591 0.99770228 0.99770228\n",
            " 0.99756458 0.99784069 0.99784069 0.99756458 0.99784069 0.99784069\n",
            " 0.49281024 0.40116321 0.38039959 0.58334489 0.5275787  0.51279092\n",
            " 0.58334489 0.5275787  0.51279092 0.99723591 0.99770228 0.99770228\n",
            " 0.99756458 0.99784069 0.99784069 0.99756458 0.99784069 0.99784069\n",
            " 0.55530321 0.47592492 0.4580474  0.76823683 0.7784613  0.77446107\n",
            " 0.76823683 0.7784613  0.77446107 0.99723591 0.99770228 0.99770228\n",
            " 0.99756458 0.99784069 0.99784069 0.99756458 0.99784069 0.99784069\n",
            " 0.30843316 0.22113406 0.20106167 0.63842024 0.65954501 0.65764994\n",
            " 0.63842024 0.65954501 0.65764994 0.99723591 0.99770228 0.99770228\n",
            " 0.99756458 0.99784069 0.99784069 0.99756458 0.99784069 0.99784069\n",
            " 0.32362291 0.29484388 0.27835    0.68953662 0.71320175 0.71269051\n",
            " 0.68953662 0.71320175 0.71269051 0.99723591 0.99770228 0.99770228\n",
            " 0.99756458 0.99784069 0.99784069 0.99756458 0.99784069 0.99784069\n",
            " 0.49281024 0.40116321 0.38039959 0.58334489 0.5275787  0.51279092\n",
            " 0.58334489 0.5275787  0.51279092 0.99723591 0.99770228 0.99770228\n",
            " 0.99756458 0.99784069 0.99784069 0.99756458 0.99784069 0.99784069\n",
            " 0.55530321 0.47592492 0.4580474  0.76823683 0.7784613  0.77446107\n",
            " 0.76823683 0.7784613  0.77446107 0.99723591 0.99770228 0.99770228\n",
            " 0.99756458 0.99784069 0.99784069 0.99756458 0.99784069 0.99784069\n",
            " 0.30843316 0.22113406 0.20106167 0.63842024 0.65954501 0.65764994\n",
            " 0.63842024 0.65954501 0.65764994 0.99723591 0.99770228 0.99770228\n",
            " 0.99756458 0.99784069 0.99784069 0.99756458 0.99784069 0.99784069\n",
            " 0.32362291 0.29484388 0.27835    0.68953662 0.71320175 0.71269051\n",
            " 0.68953662 0.71320175 0.71269051 0.99723591 0.99770228 0.99770228\n",
            " 0.99756458 0.99784069 0.99784069 0.99756458 0.99784069 0.99784069]\n",
            "Mean validation scores: [0.30406416 0.23381372 0.22288764 0.3967972  0.36270829 0.36003685\n",
            " 0.3967972  0.36270829 0.36003685 0.45169094 0.38798096 0.37647288\n",
            " 0.40040163 0.35716065 0.34756693 0.40040163 0.35716065 0.34756693\n",
            " 0.32624954 0.25174417 0.23854894 0.59356057 0.61609346 0.61690853\n",
            " 0.59356057 0.61609346 0.61690853 0.37312724 0.30953969 0.30109056\n",
            " 0.61052402 0.62934277 0.62837834 0.61052402 0.62934277 0.62837834\n",
            " 0.23244425 0.1650914  0.15395218 0.54394996 0.57129111 0.57013033\n",
            " 0.54394996 0.57129111 0.57013033 0.36275886 0.29559062 0.28828532\n",
            " 0.58793125 0.6031649  0.59880192 0.58793125 0.6031649  0.59880192\n",
            " 0.23735064 0.22100986 0.21540526 0.59720931 0.62622858 0.62665629\n",
            " 0.59720931 0.62622858 0.62665629 0.30530561 0.30178386 0.29927714\n",
            " 0.61758008 0.63926302 0.64036329 0.61758008 0.63926302 0.64036329\n",
            " 0.30406416 0.23381372 0.22288764 0.3967972  0.36270829 0.36003685\n",
            " 0.3967972  0.36270829 0.36003685 0.45169094 0.38798096 0.37647288\n",
            " 0.40040163 0.35716065 0.34756693 0.40040163 0.35716065 0.34756693\n",
            " 0.32624954 0.25174417 0.23854894 0.59356057 0.61609346 0.61690853\n",
            " 0.59356057 0.61609346 0.61690853 0.37312724 0.30953969 0.30109056\n",
            " 0.61052402 0.62934277 0.62837834 0.61052402 0.62934277 0.62837834\n",
            " 0.23244425 0.1650914  0.15395218 0.54394996 0.57129111 0.57013033\n",
            " 0.54394996 0.57129111 0.57013033 0.36275886 0.29559062 0.28828532\n",
            " 0.58793125 0.6031649  0.59880192 0.58793125 0.6031649  0.59880192\n",
            " 0.23735064 0.22100986 0.21540526 0.59720931 0.62622858 0.62665629\n",
            " 0.59720931 0.62622858 0.62665629 0.30530561 0.30178386 0.29927714\n",
            " 0.61758008 0.63926302 0.64036329 0.61758008 0.63926302 0.64036329\n",
            " 0.30406416 0.23381372 0.22288764 0.3967972  0.36270829 0.36003685\n",
            " 0.3967972  0.36270829 0.36003685 0.45169094 0.38798096 0.37647288\n",
            " 0.40040163 0.35716065 0.34756693 0.40040163 0.35716065 0.34756693\n",
            " 0.32624954 0.25174417 0.23854894 0.59356057 0.61609346 0.61690853\n",
            " 0.59356057 0.61609346 0.61690853 0.37312724 0.30953969 0.30109056\n",
            " 0.61052402 0.62934277 0.62837834 0.61052402 0.62934277 0.62837834\n",
            " 0.23244425 0.1650914  0.15395218 0.54394996 0.57129111 0.57013033\n",
            " 0.54394996 0.57129111 0.57013033 0.36275886 0.29559062 0.28828532\n",
            " 0.58793125 0.6031649  0.59880192 0.58793125 0.6031649  0.59880192\n",
            " 0.23735064 0.22100986 0.21540526 0.59720931 0.62622858 0.62665629\n",
            " 0.59720931 0.62622858 0.62665629 0.30530561 0.30178386 0.29927714\n",
            " 0.61758008 0.63926302 0.64036329 0.61758008 0.63926302 0.64036329]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/base.py:216: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe1XdVYjItmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}