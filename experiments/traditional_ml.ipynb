{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "traditional_ml.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vondersam/sdgs_text_classifier/blob/master/experiments/traditional_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6p8vLIwha9i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "54bbbfa3-2a03-41b5-8a5f-bc792a12b08a"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbXFaXGGhr21",
        "colab_type": "code",
        "outputId": "d3c8c505-ed65-417a-8eb8-8066b12237ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "base_dir = \"gdrive/My Drive/fastai-v3/sdgs/dataset/\"\n",
        "labelled_dataset = base_dir + \"cleanup_labelled.csv\"\n",
        "unlabelled_dataset = base_dir + \"cleanup_unlabelled_u.csv\""
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEVes-M5h8r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labelled = pd.read_csv(labelled_dataset)\n",
        "labelled.labels = labelled.labels.str.split('|').apply(lambda x: [int(i) for i in x])\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "data_y = mlb.fit_transform(labelled.labels)\n",
        "data_x = labelled[['text']].values\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBklIRcbhscE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the data, leave 1/3 out for testing\n",
        "stratified_split = ShuffleSplit(n_splits=2, test_size=0.33)    \n",
        "for train_index, test_index in stratified_split.split(data_x, data_y):\n",
        "    x_train, x_test = data_x[train_index], data_x[test_index]\n",
        "    y_train, y_test = data_y[train_index], data_y[test_index]\n",
        "\n",
        "\n",
        "#transform matrix of plots into lists to pass to a TfidfVectorizer\n",
        "train_x = [x[0] for x in x_train.tolist()]\n",
        "test_x = [x[0] for x in x_test.tolist()]\n",
        "\n",
        "train_y = [y[0] for y in y_train.tolist()]\n",
        "test_y = [y[0] for y in y_test.tolist()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUf7LEcxiS31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = list(range(1,18))\n",
        "\n",
        "def grid_search(train_x, train_y, test_x, test_y, labels, parameters, pipeline):\n",
        "    '''Train pipeline, test and print results'''\n",
        "    grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=3, verbose=10)\n",
        "    grid_search_tune.fit(train_x, train_y)\n",
        "\n",
        "    print()\n",
        "    print(\"Best parameters set:\")\n",
        "    print(grid_search_tune.best_estimator_.steps)\n",
        "    print()\n",
        "\n",
        "    # measuring performance on test set\n",
        "    print(\"Applying best classifier on test data:\")\n",
        "    best_clf = grid_search_tune.best_estimator_\n",
        "    predictions = best_clf.predict(test_x)\n",
        "\n",
        "    print(classification_report(test_y, predictions, target_names=labels))\n",
        "    print(\"ROC-AUC:\", roc_auc_score(test_y, predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfjWBDTr-NdK",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXmcCcXm9y8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8f9836f-2122-4d1a-9db0-ace84fb813e3"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
        "                    fit_prior=True, class_prior=None))),\n",
        "            ])\n",
        "parameters = {\n",
        "                'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "                'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "                'clf__estimator__alpha': (1e-2, 1e-3)\n",
        "            }\n",
        "grid_search(train_x, y_train, test_x, y_test, mlb.classes, parameters, pipeline)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 18 candidates, totalling 36 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
            "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    3.5s\n",
            "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    7.2s\n",
            "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   11.2s\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   17.0s\n",
            "[Parallel(n_jobs=3)]: Done  36 out of  36 | elapsed:   23.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Best parameters set:\n",
            "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.25, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
            "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
            "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
            "                            'been', 'before', 'being', 'below', 'between',\n",
            "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)), ('clf', OneVsRestClassifier(estimator=MultinomialNB(alpha=0.01, class_prior=None,\n",
            "                                            fit_prior=True),\n",
            "                    n_jobs=None))]\n",
            "\n",
            "Applying best classifier on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.33      0.45       145\n",
            "           1       0.80      0.44      0.57       154\n",
            "           2       0.74      0.54      0.63       162\n",
            "           3       0.84      0.50      0.63       130\n",
            "           4       0.71      0.46      0.56       178\n",
            "           5       0.68      0.34      0.45       146\n",
            "           6       0.81      0.56      0.66       140\n",
            "           7       0.81      0.34      0.48       143\n",
            "           8       0.80      0.41      0.54       101\n",
            "           9       0.84      0.46      0.59        90\n",
            "          10       0.96      0.57      0.72       122\n",
            "          11       0.87      0.53      0.66       113\n",
            "          12       0.79      0.36      0.49       107\n",
            "          13       0.90      0.81      0.85       171\n",
            "          14       0.85      0.58      0.69       131\n",
            "          15       0.86      0.56      0.68       148\n",
            "          16       0.86      0.63      0.72       182\n",
            "\n",
            "   micro avg       0.82      0.50      0.62      2363\n",
            "   macro avg       0.81      0.49      0.61      2363\n",
            "weighted avg       0.81      0.50      0.61      2363\n",
            " samples avg       0.61      0.61      0.60      2363\n",
            "\n",
            "ROC-AUC: 0.74225267234477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6np7GSQXErBx",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pMWIkVZEf2m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9f3a370-8162-471d-e6bc-79ccb9913768"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "    ('clf', OneVsRestClassifier(LinearSVC())),\n",
        "])\n",
        "parameters = {\n",
        "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    \"clf__estimator__C\": [0.01, 0.1, 1],\n",
        "    \"clf__estimator__class_weight\": ['balanced', None],\n",
        "}\n",
        "grid_search(train_x, y_train, test_x, y_test, mlb.classes, parameters, pipeline)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
            "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    4.6s\n",
            "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    8.0s\n",
            "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   12.1s\n",
            "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   16.8s\n",
            "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:   23.6s\n",
            "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   29.9s\n",
            "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:   39.5s\n",
            "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:   47.4s\n",
            "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=3)]: Done 108 out of 108 | elapsed:  1.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Best parameters set:\n",
            "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
            "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
            "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
            "                            'been', 'before', 'being', 'below', 'between',\n",
            "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LinearSVC(C=1, class_weight='balanced', dual=True,\n",
            "                                        fit_intercept=True, intercept_scaling=1,\n",
            "                                        loss='squared_hinge', max_iter=1000,\n",
            "                                        multi_class='ovr', penalty='l2',\n",
            "                                        random_state=None, tol=0.0001,\n",
            "                                        verbose=0),\n",
            "                    n_jobs=None))]\n",
            "\n",
            "Applying best classifier on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.55      0.62       145\n",
            "           1       0.75      0.54      0.63       154\n",
            "           2       0.75      0.60      0.66       162\n",
            "           3       0.79      0.62      0.70       130\n",
            "           4       0.72      0.54      0.62       178\n",
            "           5       0.78      0.52      0.62       146\n",
            "           6       0.81      0.71      0.76       140\n",
            "           7       0.69      0.52      0.59       143\n",
            "           8       0.60      0.43      0.50       101\n",
            "           9       0.89      0.66      0.76        90\n",
            "          10       0.94      0.80      0.87       122\n",
            "          11       0.86      0.78      0.82       113\n",
            "          12       0.82      0.69      0.75       107\n",
            "          13       0.96      0.89      0.93       171\n",
            "          14       0.94      0.73      0.82       131\n",
            "          15       0.86      0.72      0.78       148\n",
            "          16       0.87      0.81      0.84       182\n",
            "\n",
            "   micro avg       0.81      0.66      0.73      2363\n",
            "   macro avg       0.81      0.65      0.72      2363\n",
            "weighted avg       0.81      0.66      0.72      2363\n",
            " samples avg       0.73      0.72      0.71      2363\n",
            "\n",
            "ROC-AUC: 0.8202819422032758\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlXiPkDuEytV",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_Lao8-DEnN_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b31d252-286e-46bf-ceb2-fded2dc013dd"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "    ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'))),\n",
        "])\n",
        "parameters = {\n",
        "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    \"clf__estimator__C\": [0.01, 0.1, 1],\n",
        "    \"clf__estimator__class_weight\": ['balanced', None],\n",
        "}\n",
        "grid_search(train_x, y_train, test_x, y_test, mlb.classes, parameters, pipeline)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    2.2s\n",
            "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:   15.0s\n",
            "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   27.7s\n",
            "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   49.0s\n",
            "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:  2.8min\n",
            "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:  3.5min\n",
            "[Parallel(n_jobs=3)]: Done 108 out of 108 | elapsed:  3.9min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Best parameters set:\n",
            "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.25, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
            "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
            "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
            "                            'been', 'before', 'being', 'below', 'between',\n",
            "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LogisticRegression(C=1, class_weight='balanced',\n",
            "                                                 dual=False, fit_intercept=True,\n",
            "                                                 intercept_scaling=1,\n",
            "                                                 l1_ratio=None, max_iter=100,\n",
            "                                                 multi_class='warn',\n",
            "                                                 n_jobs=None, penalty='l2',\n",
            "                                                 random_state=None,\n",
            "                                                 solver='sag', tol=0.0001,\n",
            "                                                 verbose=0, warm_start=False),\n",
            "                    n_jobs=None))]\n",
            "\n",
            "Applying best classifier on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.64      0.60       145\n",
            "           1       0.74      0.60      0.67       154\n",
            "           2       0.68      0.64      0.66       162\n",
            "           3       0.77      0.65      0.70       130\n",
            "           4       0.63      0.60      0.61       178\n",
            "           5       0.76      0.54      0.63       146\n",
            "           6       0.77      0.72      0.74       140\n",
            "           7       0.64      0.62      0.63       143\n",
            "           8       0.57      0.49      0.52       101\n",
            "           9       0.85      0.70      0.77        90\n",
            "          10       0.90      0.84      0.87       122\n",
            "          11       0.83      0.81      0.82       113\n",
            "          12       0.71      0.75      0.73       107\n",
            "          13       0.93      0.91      0.92       171\n",
            "          14       0.92      0.76      0.83       131\n",
            "          15       0.83      0.71      0.77       148\n",
            "          16       0.72      0.83      0.77       182\n",
            "\n",
            "   micro avg       0.75      0.69      0.72      2363\n",
            "   macro avg       0.75      0.69      0.72      2363\n",
            "weighted avg       0.75      0.69      0.72      2363\n",
            " samples avg       0.72      0.75      0.72      2363\n",
            "\n",
            "ROC-AUC: 0.8359024877783872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukbTje5GBYVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}