{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "traditional_ml.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vondersam/sdgs_text_classifier/blob/master/experiments/traditional_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6p8vLIwha9i",
        "colab_type": "code",
        "outputId": "73dc5640-df71-4d13-ab94-2f1f38ef4832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbXFaXGGhr21",
        "colab_type": "code",
        "outputId": "6867c9e9-5560-4748-92c8-73273a1b57d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "base_dir = \"gdrive/My Drive/fastai-v3/sdgs/dataset/\"\n",
        "labelled_dataset = base_dir + \"cleanup_labelled.csv\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEVes-M5h8r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labelled = pd.read_csv(labelled_dataset)\n",
        "labelled.labels = labelled.labels.str.split('|').apply(lambda x: [int(i) for i in x])\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "data_y = mlb.fit_transform(labelled.labels)\n",
        "data_x = labelled[['text']].values\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBklIRcbhscE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the data, leave 1/3 out for testing\n",
        "stratified_split = ShuffleSplit(n_splits=2, test_size=0.33)    \n",
        "for train_index, test_index in stratified_split.split(data_x, data_y):\n",
        "    x_train, x_test = data_x[train_index], data_x[test_index]\n",
        "    y_train, y_test = data_y[train_index], data_y[test_index]\n",
        "\n",
        "\n",
        "#transform matrix of plots into lists to pass to a TfidfVectorizer\n",
        "train_x = [x[0] for x in x_train.tolist()]\n",
        "test_x = [x[0] for x in x_test.tolist()]\n",
        "\n",
        "#train_y = [[y[0] for y in y_train.tolist()]]\n",
        "#test_y = [[y[0] for y in y_test.tolist()]]\n",
        "train_y = y_train\n",
        "test_y = y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUf7LEcxiS31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = list(range(1,18))\n",
        "\n",
        "def grid_search(train_x, train_y, test_x, test_y, labels, parameters, pipeline):\n",
        "    '''Train pipeline, test and print results'''\n",
        "    grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=3, verbose=10)\n",
        "    grid_search_tune.fit(train_x, train_y)\n",
        "\n",
        "    print()\n",
        "    print(\"Best parameters set:\")\n",
        "    print(grid_search_tune.best_estimator_.steps)\n",
        "    print()\n",
        "\n",
        "    # measuring performance on test set\n",
        "    print(\"Applying best classifier on test data:\")\n",
        "    best_clf = grid_search_tune.best_estimator_\n",
        "    predictions = best_clf.predict(test_x)\n",
        "\n",
        "    print(classification_report(test_y, predictions, target_names=labels))\n",
        "    print(\"ROC-AUC:\", roc_auc_score(test_y, predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfjWBDTr-NdK",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXmcCcXm9y8f",
        "colab_type": "code",
        "outputId": "3061cea0-e8a9-423b-ee0c-fe0c5c87aa73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "labels = [str(i) for i in range(1,18)]\n",
        "pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
        "                    fit_prior=True, class_prior=None))),\n",
        "            ])\n",
        "parameters = {\n",
        "                'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "                'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "                'clf__estimator__alpha': (1e-2, 1e-3)\n",
        "            }\n",
        "grid_search(train_x, y_train, test_x, y_test, labels, parameters, pipeline)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 18 candidates, totalling 36 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
            "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.9s\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    4.0s\n",
            "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    8.8s\n",
            "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   13.6s\n",
            "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   18.4s\n",
            "[Parallel(n_jobs=3)]: Done  36 out of  36 | elapsed:   25.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Best parameters set:\n",
            "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.25, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
            "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
            "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
            "                            'been', 'before', 'being', 'below', 'between',\n",
            "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)), ('clf', OneVsRestClassifier(estimator=MultinomialNB(alpha=0.01, class_prior=None,\n",
            "                                            fit_prior=True),\n",
            "                    n_jobs=None))]\n",
            "\n",
            "Applying best classifier on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.73      0.38      0.50       125\n",
            "           2       0.88      0.41      0.56       143\n",
            "           3       0.83      0.63      0.72       172\n",
            "           4       0.82      0.54      0.65       116\n",
            "           5       0.75      0.52      0.62       168\n",
            "           6       0.65      0.41      0.50       138\n",
            "           7       0.85      0.61      0.71       145\n",
            "           8       0.84      0.41      0.55       141\n",
            "           9       0.88      0.39      0.54        95\n",
            "          10       0.83      0.60      0.69        89\n",
            "          11       0.84      0.62      0.72       120\n",
            "          12       0.86      0.55      0.67       118\n",
            "          13       0.85      0.49      0.63        95\n",
            "          14       0.97      0.80      0.88       178\n",
            "          15       0.88      0.62      0.72       136\n",
            "          16       0.87      0.70      0.77       150\n",
            "          17       0.84      0.61      0.71       183\n",
            "\n",
            "   micro avg       0.84      0.56      0.67      2312\n",
            "   macro avg       0.83      0.55      0.65      2312\n",
            "weighted avg       0.83      0.56      0.66      2312\n",
            " samples avg       0.65      0.65      0.64      2312\n",
            "\n",
            "ROC-AUC: 0.7688389827279807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6np7GSQXErBx",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pMWIkVZEf2m",
        "colab_type": "code",
        "outputId": "223aae46-fba5-48e6-aa1b-b42f1b31a303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "    ('clf', OneVsRestClassifier(LinearSVC())),\n",
        "])\n",
        "parameters = {\n",
        "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    \"clf__estimator__C\": [0.01, 0.1, 1],\n",
        "    \"clf__estimator__class_weight\": ['balanced', None],\n",
        "}\n",
        "grid_search(train_x, y_train, test_x, y_test, mlb.classes, parameters, pipeline)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
            "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    3.1s\n",
            "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    6.4s\n",
            "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    9.3s\n",
            "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   14.5s\n",
            "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   18.3s\n",
            "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:   24.5s\n",
            "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   31.3s\n",
            "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:   40.5s\n",
            "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:   48.3s\n",
            "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=3)]: Done 108 out of 108 | elapsed:  1.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Best parameters set:\n",
            "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
            "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
            "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
            "                            'been', 'before', 'being', 'below', 'between',\n",
            "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LinearSVC(C=1, class_weight='balanced', dual=True,\n",
            "                                        fit_intercept=True, intercept_scaling=1,\n",
            "                                        loss='squared_hinge', max_iter=1000,\n",
            "                                        multi_class='ovr', penalty='l2',\n",
            "                                        random_state=None, tol=0.0001,\n",
            "                                        verbose=0),\n",
            "                    n_jobs=None))]\n",
            "\n",
            "Applying best classifier on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.58      0.63       125\n",
            "           1       0.77      0.52      0.62       143\n",
            "           2       0.74      0.61      0.67       172\n",
            "           3       0.79      0.71      0.75       116\n",
            "           4       0.75      0.57      0.65       168\n",
            "           5       0.75      0.65      0.70       138\n",
            "           6       0.83      0.70      0.76       145\n",
            "           7       0.79      0.62      0.69       141\n",
            "           8       0.76      0.49      0.60        95\n",
            "           9       0.85      0.72      0.78        89\n",
            "          10       0.92      0.82      0.86       120\n",
            "          11       0.92      0.86      0.89       118\n",
            "          12       0.76      0.75      0.75        95\n",
            "          13       0.96      0.86      0.91       178\n",
            "          14       0.90      0.76      0.82       136\n",
            "          15       0.94      0.77      0.85       150\n",
            "          16       0.92      0.73      0.81       183\n",
            "\n",
            "   micro avg       0.83      0.69      0.75      2312\n",
            "   macro avg       0.83      0.69      0.75      2312\n",
            "weighted avg       0.83      0.69      0.75      2312\n",
            " samples avg       0.75      0.74      0.73      2312\n",
            "\n",
            "ROC-AUC: 0.8382826448930474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlXiPkDuEytV",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_Lao8-DEnN_",
        "colab_type": "code",
        "outputId": "e7aa39ce-cf06-4911-99e6-4a74745988ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "    ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'))),\n",
        "])\n",
        "parameters = {\n",
        "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    \"clf__estimator__C\": [0.01, 0.1, 1],\n",
        "    \"clf__estimator__class_weight\": ['balanced', None],\n",
        "    \"clf__estimator__multi_class\": ['ovr', 'multinomial']\n",
        "}\n",
        "grid_search(train_x, y_train, test_x, y_test, mlb.classes, parameters, pipeline)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 108 candidates, totalling 216 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
            "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    3.2s\n",
            "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:   17.7s\n",
            "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   29.7s\n",
            "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   51.9s\n",
            "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:  2.8min\n",
            "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:  3.9min\n",
            "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:  4.2min\n",
            "[Parallel(n_jobs=3)]: Done 107 tasks      | elapsed:  4.6min\n",
            "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:  4.9min\n",
            "[Parallel(n_jobs=3)]: Done 139 tasks      | elapsed:  5.3min\n",
            "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:  6.1min\n",
            "[Parallel(n_jobs=3)]: Done 175 tasks      | elapsed:  8.1min\n",
            "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  8.9min\n",
            "[Parallel(n_jobs=3)]: Done 216 out of 216 | elapsed:  9.5min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Best parameters set:\n",
            "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.25, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
            "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
            "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
            "                            'been', 'before', 'being', 'below', 'between',\n",
            "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LogisticRegression(C=1, class_weight='balanced',\n",
            "                                                 dual=False, fit_intercept=True,\n",
            "                                                 intercept_scaling=1,\n",
            "                                                 l1_ratio=None, max_iter=100,\n",
            "                                                 multi_class='ovr', n_jobs=None,\n",
            "                                                 penalty='l2',\n",
            "                                                 random_state=None,\n",
            "                                                 solver='sag', tol=0.0001,\n",
            "                                                 verbose=0, warm_start=False),\n",
            "                    n_jobs=None))]\n",
            "\n",
            "Applying best classifier on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.55      0.58       135\n",
            "           1       0.77      0.57      0.66       171\n",
            "           2       0.73      0.62      0.67       172\n",
            "           3       0.89      0.62      0.73       135\n",
            "           4       0.56      0.61      0.58       157\n",
            "           5       0.75      0.61      0.68       124\n",
            "           6       0.80      0.77      0.79       153\n",
            "           7       0.71      0.65      0.68       136\n",
            "           8       0.63      0.58      0.60        95\n",
            "           9       0.88      0.68      0.76       108\n",
            "          10       0.90      0.84      0.87       123\n",
            "          11       0.86      0.84      0.85       115\n",
            "          12       0.62      0.75      0.68       114\n",
            "          13       0.92      0.87      0.90       172\n",
            "          14       0.90      0.78      0.84       120\n",
            "          15       0.89      0.73      0.80       141\n",
            "          16       0.82      0.76      0.79       208\n",
            "\n",
            "   micro avg       0.77      0.70      0.73      2379\n",
            "   macro avg       0.78      0.70      0.73      2379\n",
            "weighted avg       0.78      0.70      0.73      2379\n",
            " samples avg       0.73      0.76      0.73      2379\n",
            "\n",
            "ROC-AUC: 0.8390236207218793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvVCq_YGFH8R",
        "colab_type": "text"
      },
      "source": [
        "# Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7CSwel9FJfd",
        "colab_type": "code",
        "outputId": "0ec97a32-e41e-4d08-a33f-a00f6f6b1b65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "    ('clf', DecisionTreeClassifier()),\n",
        "])\n",
        "parameters = {\n",
        "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]\n",
        "}\n",
        "grid_search(train_x, y_train, test_x, y_test, mlb.classes, parameters, pipeline)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    2.3s\n",
            "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:   16.3s\n",
            "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   28.7s\n",
            "[Parallel(n_jobs=3)]: Done  15 out of  18 | elapsed:   36.3s remaining:    7.3s\n",
            "[Parallel(n_jobs=3)]: Done  18 out of  18 | elapsed:   45.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Best parameters set:\n",
            "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
            "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
            "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
            "                            'been', 'before', 'being', 'below', 'between',\n",
            "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
            "                       max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort=False,\n",
            "                       random_state=None, splitter='best'))]\n",
            "\n",
            "Applying best classifier on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.32      0.37       150\n",
            "           1       0.48      0.45      0.47       140\n",
            "           2       0.55      0.47      0.51       167\n",
            "           3       0.75      0.44      0.55       148\n",
            "           4       0.42      0.54      0.47       158\n",
            "           5       0.44      0.52      0.48       137\n",
            "           6       0.70      0.67      0.68       150\n",
            "           7       0.48      0.31      0.38       136\n",
            "           8       0.46      0.34      0.39        97\n",
            "           9       0.44      0.38      0.41        99\n",
            "          10       0.80      0.76      0.78       134\n",
            "          11       0.64      0.65      0.65       106\n",
            "          12       0.72      0.66      0.69       111\n",
            "          13       0.83      0.88      0.85       162\n",
            "          14       0.79      0.63      0.70       130\n",
            "          15       0.70      0.62      0.66       152\n",
            "          16       0.68      0.68      0.68       199\n",
            "\n",
            "   micro avg       0.61      0.56      0.58      2376\n",
            "   macro avg       0.61      0.55      0.57      2376\n",
            "weighted avg       0.61      0.56      0.58      2376\n",
            " samples avg       0.64      0.62      0.61      2376\n",
            "\n",
            "ROC-AUC: 0.7581271522043124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqw-DqlqIV8C",
        "colab_type": "text"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE_pyE6ZF0jM",
        "colab_type": "code",
        "outputId": "da7938a7-c4b0-4a69-f47b-7a3d39e0ea4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "    ('clf', KNeighborsClassifier()),\n",
        "])\n",
        "parameters = {\n",
        "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    'clf__n_neighbors': (2,3,4,5),\n",
        "    'clf__weights': ('uniform', 'distance'),\n",
        "    'clf__metric': ['minkowski'],\n",
        "    'clf__algorithm': ('ball_tree', 'kd_tree', 'brute')\n",
        "}\n",
        "grid_search(train_x, y_train, test_x, y_test, mlb.classes, parameters, pipeline)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 216 candidates, totalling 432 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
            "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    5.0s\n",
            "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:   11.5s\n",
            "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   18.8s\n",
            "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   26.7s\n",
            "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   30.6s\n",
            "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:   36.7s\n",
            "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   48.6s\n",
            "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=3)]: Done 107 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=3)]: Done 139 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=3)]: Done 175 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  3.3min\n",
            "[Parallel(n_jobs=3)]: Done 215 tasks      | elapsed:  3.5min\n",
            "[Parallel(n_jobs=3)]: Done 236 tasks      | elapsed:  4.0min\n",
            "[Parallel(n_jobs=3)]: Done 259 tasks      | elapsed:  4.3min\n",
            "[Parallel(n_jobs=3)]: Done 282 tasks      | elapsed:  4.7min\n",
            "[Parallel(n_jobs=3)]: Done 307 tasks      | elapsed:  5.1min\n",
            "[Parallel(n_jobs=3)]: Done 332 tasks      | elapsed:  5.5min\n",
            "[Parallel(n_jobs=3)]: Done 359 tasks      | elapsed:  5.9min\n",
            "[Parallel(n_jobs=3)]: Done 386 tasks      | elapsed:  6.4min\n",
            "[Parallel(n_jobs=3)]: Done 415 tasks      | elapsed:  6.9min\n",
            "[Parallel(n_jobs=3)]: Done 432 out of 432 | elapsed:  7.1min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/base.py:216: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Best parameters set:\n",
            "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
            "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
            "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
            "                            'been', 'before', 'being', 'below', 'between',\n",
            "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)), ('clf', KNeighborsClassifier(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
            "                     weights='distance'))]\n",
            "\n",
            "Applying best classifier on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.32      0.46       125\n",
            "           1       0.94      0.34      0.50       143\n",
            "           2       0.87      0.53      0.66       172\n",
            "           3       0.84      0.44      0.58       116\n",
            "           4       0.45      0.68      0.54       168\n",
            "           5       0.53      0.64      0.58       138\n",
            "           6       0.90      0.51      0.65       145\n",
            "           7       0.89      0.44      0.59       141\n",
            "           8       0.80      0.35      0.49        95\n",
            "           9       0.78      0.48      0.60        89\n",
            "          10       0.95      0.61      0.74       120\n",
            "          11       0.92      0.46      0.61       118\n",
            "          12       0.84      0.45      0.59        95\n",
            "          13       0.97      0.70      0.81       178\n",
            "          14       0.97      0.53      0.69       136\n",
            "          15       0.97      0.60      0.74       150\n",
            "          16       0.88      0.55      0.68       183\n",
            "\n",
            "   micro avg       0.78      0.52      0.63      2312\n",
            "   macro avg       0.84      0.51      0.62      2312\n",
            "weighted avg       0.84      0.52      0.63      2312\n",
            " samples avg       0.63      0.61      0.61      2312\n",
            "\n",
            "ROC-AUC: 0.7479258538300472\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe1XdVYjItmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}