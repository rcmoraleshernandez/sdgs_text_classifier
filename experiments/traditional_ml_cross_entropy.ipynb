{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "traditional_ml_cross_entropy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vondersam/sdgs_text_classifier/blob/master/experiments/traditional_ml_cross_entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB6aDzb72Lz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6p8vLIwha9i",
        "colab_type": "code",
        "outputId": "f0ff7c98-bf0a-4e91-ec18-35ba7c710f3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import string\n",
        "import os\n",
        "\n",
        "### SKLEARN ###\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, hamming_loss, accuracy_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.decomposition import PCA, NMF\n",
        "\n",
        "### NLTK ###\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk import sent_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "\n",
        "### SPACY ###\n",
        "#import spacy\n",
        "#spacy_stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "#nlp = spacy.load('en_core_web_lg', disable=['ner', 'parser'])\n",
        "\n",
        "from joblib import dump, load\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbXFaXGGhr21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "base_dir = \"gdrive/My Drive/fastai-v3/sdgs/\"\n",
        "labelled_dataset = base_dir + \"dataset/cleanup_labelled.csv\"\n",
        "CROSS_FOLDS = f\"{base_dir}dataset/cross_validation/\"\n",
        "OUTPUT_DIR = f\"{base_dir}traditional_ml/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEVes-M5h8r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(labelled_dataset)\n",
        "df.labels = df.labels.str.split('|').apply(lambda x: [int(i) for i in x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq-Npyjn70jj",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwDFmhd99uVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpacyPreprocessor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stopwords  = spacy_stop_words\n",
        "        self.punct      = set(string.punctuation)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def inverse_transform(self, X):\n",
        "        return [\" \".join(doc) for doc in X]\n",
        "\n",
        "    def transform(self, X):\n",
        "        return [\n",
        "            list(self.tokenize(doc)) for doc in X\n",
        "        ]\n",
        "\n",
        "    def tokenize(self, document):\n",
        "        for token in nlp(document):\n",
        "\n",
        "            # Disregard stopwords\n",
        "            if token in self.stopwords:\n",
        "                continue\n",
        "\n",
        "            # Disregard punctuation\n",
        "            if all(char in self.punct for char in token.text):\n",
        "                continue\n",
        "\n",
        "            # yield lemmatized tokens\n",
        "            yield token.lemma_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoc981iI6psQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, stopwords=None, punct=None,\n",
        "                 lower=True, strip=True):\n",
        "        self.stopwords  = set(sw.words('english'))\n",
        "        self.punct      = set(string.punctuation)\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def inverse_transform(self, X):\n",
        "        return [\" \".join(doc) for doc in X]\n",
        "\n",
        "    def transform(self, X):\n",
        "        return [\n",
        "            list(self.tokenize(doc)) for doc in X\n",
        "        ]\n",
        "\n",
        "    def tokenize(self, document):\n",
        "        for token, tag in pos_tag(word_tokenize(document)):\n",
        "            token = token.lower()\n",
        "            token = token.strip()\n",
        "            token = token.strip('_')\n",
        "            token = token.strip('*')\n",
        "\n",
        "            # Disregard stopwords\n",
        "            if token in self.stopwords:\n",
        "                continue\n",
        "\n",
        "            # Disregard punctuation\n",
        "            if all(char in self.punct for char in token):\n",
        "                continue\n",
        "\n",
        "            # yield lemmatized tokens\n",
        "            lemma = self.lemmatize(token, tag)\n",
        "            yield lemma\n",
        "\n",
        "    def lemmatize(self, token, tag):\n",
        "        tag = {\n",
        "            'N': wn.NOUN,\n",
        "            'V': wn.VERB,\n",
        "            'R': wn.ADV,\n",
        "            'J': wn.ADJ\n",
        "        }.get(tag[0], wn.NOUN)\n",
        "        return self.lemmatizer.lemmatize(token, tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGWKkATC9Wav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity(arg):\n",
        "    \"\"\"\n",
        "    Simple identity function works as a passthrough.\n",
        "    \"\"\"\n",
        "    return arg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAWjeu2n7rNU",
        "colab_type": "text"
      },
      "source": [
        "# Pipeline with preprocessor, vectorizer and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDT5rTaHV9jU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_classifier(train_x, train_y, arch, preprocessor=NLTKPreprocessor()):\n",
        "    if arch == 'svm':\n",
        "        clf = OneVsRestClassifier(estimator=LinearSVC(C=1, class_weight='balanced', dual=True,\n",
        "                                            fit_intercept=True, intercept_scaling=1,\n",
        "                                            loss='squared_hinge', max_iter=1000,\n",
        "                                            multi_class='ovr', penalty='l2',\n",
        "                                            random_state=None, tol=0.0001,\n",
        "                                            verbose=0))\n",
        "\n",
        "        word_vectorizer = TfidfVectorizer(binary=False, decode_error='strict',\n",
        "                    encoding='utf-8', dtype=np.float64,\n",
        "                    input='content', lowercase=False, max_df=0.25, max_features=None,\n",
        "                    min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
        "                    smooth_idf=True,\n",
        "                    stop_words=None,\n",
        "                    strip_accents=None, sublinear_tf=False,\n",
        "                    tokenizer=identity, use_idf=True,\n",
        "                    vocabulary=None) \n",
        "        \n",
        "\n",
        "    elif arch == 'nb':\n",
        "        clf = OneVsRestClassifier(estimator=MultinomialNB(alpha=0.01, class_prior=None,\n",
        "                                                fit_prior=True))\n",
        "\n",
        "        word_vectorizer = TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
        "                    dtype=np.float64, encoding='utf-8',\n",
        "                    input='content', lowercase=False, max_df=0.25, max_features=None,\n",
        "                    min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
        "                    smooth_idf=True,\n",
        "                    stop_words=None,\n",
        "                    strip_accents=None, sublinear_tf=False,\n",
        "                    tokenizer=identity, use_idf=True,\n",
        "                    vocabulary=None)\n",
        "        \n",
        "    \n",
        "    elif arch == 'lg':\n",
        "        clf = OneVsRestClassifier(estimator=LogisticRegression(C=1, class_weight='balanced',\n",
        "                                                     dual=False, fit_intercept=True,\n",
        "                                                     intercept_scaling=1,\n",
        "                                                     l1_ratio=None, max_iter=4000,\n",
        "                                                     multi_class='ovr',\n",
        "                                                     n_jobs=None, penalty='l2',\n",
        "                                                     random_state=None,\n",
        "                                                     solver='sag', tol=0.0001,\n",
        "                                                     verbose=0, warm_start=False),\n",
        "                                                     n_jobs=None)\n",
        "\n",
        "        word_vectorizer = TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
        "                    dtype=np.float64, encoding='utf-8',\n",
        "                    input='content', lowercase=False, max_df=0.25, max_features=None,\n",
        "                    min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
        "                    smooth_idf=True,\n",
        "                    stop_words=None,\n",
        "                    strip_accents=None, sublinear_tf=False,\n",
        "                    tokenizer=identity, use_idf=True,\n",
        "                    vocabulary=None)\n",
        "        \n",
        "    \n",
        "    elif arch == 'knn':\n",
        "        clf = KNeighborsClassifier(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
        "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
        "                     weights='distance')\n",
        "        word_vectorizer = TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
        "                dtype=np.float64, encoding='utf-8',\n",
        "                input='content', lowercase=False, max_df=0.5, max_features=None,\n",
        "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
        "                stop_words=None,\n",
        "                strip_accents=None, sublinear_tf=False,\n",
        "                tokenizer=identity, use_idf=True,\n",
        "                vocabulary=None) \n",
        "        \n",
        "\n",
        "    pipe = Pipeline([('preprocessor', preprocessor), ('tfidf', word_vectorizer), ('multilabel', clf)])\n",
        "    pipe.fit(train_x, train_y)\n",
        "    return pipe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5MfhEZJ7lbu",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwmb4EOCI-Gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_avg(models_testx_testy, labels_):\n",
        "    def calc(model, test_x, test_y):\n",
        "        predictions = model.predict(test_x)\n",
        "        metrics = classification_report(test_y, predictions, target_names=labels_, output_dict=True)\n",
        "        metrics_df = pd.DataFrame.from_dict(metrics)\n",
        "        h = hamming_loss(test_y, predictions)\n",
        "        roc = roc_auc_score(test_y, predictions, average='micro')\n",
        "        return metrics_df, h, roc\n",
        "    \n",
        "    model_1, test_x_first, test_y_first = models_testx_testy[0]\n",
        "    metrics_agg, ham, roc = calc(model_1, test_x_first, test_y_1_first)\n",
        "    n = len(models_testx_testy)\n",
        "  \n",
        "    for model, test_x, test_y in models_testx_testy[1:]:\n",
        "        metrics, h, r = calc(model, test_x, test_y)\n",
        "        metrics_agg += metrics\n",
        "        ham += h\n",
        "        roc += r\n",
        "\n",
        "    return metrics_agg/n, ham/n, roc/n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmwDCosdpw6Q",
        "colab_type": "text"
      },
      "source": [
        "# Train the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMeghBn3zxg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "x = df[['text']].values # text\n",
        "y = mlb.fit_transform(df.labels) # labels\n",
        "labels = [str(i) for i in range(1,18)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EIY5pZzCMNLt",
        "colab": {}
      },
      "source": [
        "arch = 'knn'\n",
        "\n",
        "\n",
        "models = []\n",
        "for fold in os.listdir(CROSS_FOLDS):\n",
        "    # Load predefined indices for train, val and test\n",
        "    train_index = np.load(f\"{CROSS_FOLDS}{fold}/train.npy\")\n",
        "    #val_index = np.load(f\"{CROSS_FOLDS}{fold}/val.npy\")\n",
        "    test_index = np.load(f\"{CROSS_FOLDS}{fold}/test.npy\")\n",
        "\n",
        "    print(fold)\n",
        "\n",
        "    # Load train and test texts\n",
        "    x_train = [t[0] for t in x[train_index].tolist()]\n",
        "    #x_val = [t[0] for t in x[val_index].tolist()]\n",
        "    x_test = [t[0] for t in x[test_index].tolist()]\n",
        "\n",
        "    # Load train and test labels\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Fit model on fold data\n",
        "    model = run_classifier(x_train, y_train, arch=arch)\n",
        "    models.append((model, x_test, y_test))\n",
        "\n",
        "    # Save model\n",
        "    save_dir = Path(f\"{OUTPUT_DIR}{arch}/\")\n",
        "    save_dir.mkdir(exist_ok=True)\n",
        "    file_dir = save_dir/f\"{arch}_{fold}.joblib\"\n",
        "    dump(model, file_dir)\n",
        "print(f\"Finished trainin {arch}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dshrJceYb1O",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XoaaCI8LCQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = metrics_avg(models, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqf-y3RWNpT8",
        "colab_type": "code",
        "outputId": "886f8105-ba8f-487b-b674-368fd928a38f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results[2]"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7635767039814408"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlm6H2vmYEWD",
        "colab_type": "text"
      },
      "source": [
        "# Load and evaluate saved models on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywBE6Hdu4bQ4",
        "colab_type": "code",
        "outputId": "521a3d89-244f-4b80-cbaa-308a51921c32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loaded_arch = 'nb'\n",
        "loaded_models = []\n",
        "for fold in os.listdir(CROSS_FOLDS):\n",
        "    test_index = np.load(f\"{CROSS_FOLDS}{fold}/test.npy\")\n",
        "\n",
        "    x_test = [t[0] for t in x[test_index].tolist()]\n",
        "    y_test = y[test_index]\n",
        "    \n",
        "    load_dir = Path(f\"{OUTPUT_DIR}{loaded_arch}/\")\n",
        "    load_dir = load_dir/f\"{loaded_arch}_{fold}.joblib\"\n",
        "    \n",
        "    loaded_model = load(load_dir)\n",
        "    loaded_models.append((loaded_model, x_test, y_test))\n",
        "print(f\"Finished loading the {loaded_arch} models.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished loading the nb models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gfqorniS-7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Obtaining results for {loaded_arch}: \")\n",
        "loaded_results = metrics_avg(loaded_models, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv5nabqyijWf",
        "colab_type": "code",
        "outputId": "27cc22df-3884-4ba6-d8e1-e676fc5687e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loaded_results[2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7915714104037355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    }
  ]
}