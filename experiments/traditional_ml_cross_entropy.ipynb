{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "traditional_ml_cross_entropy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vondersam/sdgs_text_classifier/blob/master/experiments/traditional_ml_cross_entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB6aDzb72Lz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6p8vLIwha9i",
        "colab_type": "code",
        "outputId": "193e032e-a5bc-4f2f-da8c-ddc40500b78b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import string\n",
        "import os\n",
        "import re\n",
        "\n",
        "### SKLEARN ###\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, hamming_loss, accuracy_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "### NLTK ###\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "\n",
        "### SPACY ###\n",
        "#import spacy\n",
        "#spacy_stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "#nlp = spacy.load('en_core_web_lg', disable=['ner', 'parser'])\n",
        "\n",
        "from joblib import dump, load\n",
        "from pathlib import Path"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbXFaXGGhr21",
        "colab_type": "code",
        "outputId": "9459dbeb-0e65-48a8-d47f-f5148176f74c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "base_dir = \"gdrive/My Drive/fastai-v3/sdgs/\"\n",
        "\n",
        "CROSS_FOLDS = f\"{base_dir}dataset/cross_validation/\"\n",
        "OUTPUT_DIR = f\"{base_dir}traditional_ml/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq-Npyjn70jj",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwDFmhd99uVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpacyPreprocessor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stopwords  = spacy_stop_words\n",
        "        self.punct      = set(string.punctuation)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def inverse_transform(self, X):\n",
        "        return [\" \".join(doc) for doc in X]\n",
        "\n",
        "    def transform(self, X):\n",
        "        return [\n",
        "            list(self.tokenize(doc)) for doc in X\n",
        "        ]\n",
        "\n",
        "    def tokenize(self, document):\n",
        "        for token in nlp(document):\n",
        "\n",
        "            # Disregard stopwords\n",
        "            if token in self.stopwords:\n",
        "                continue\n",
        "\n",
        "            # Disregard punctuation\n",
        "            if all(char in self.punct for char in token.text):\n",
        "                continue\n",
        "\n",
        "            # yield lemmatized tokens\n",
        "            yield token.lemma_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoc981iI6psQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, stopwords=None, punct=None,\n",
        "                 lower=True, strip=True):\n",
        "        self.stopwords  = set(sw.words('english'))\n",
        "        self.punct      = set(string.punctuation)\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def inverse_transform(self, X):\n",
        "        return [\" \".join(doc) for doc in X]\n",
        "\n",
        "    def transform(self, X):\n",
        "        return [\n",
        "            list(self.tokenize(doc)) for doc in X\n",
        "        ]\n",
        "\n",
        "    def tokenize(self, document):\n",
        "        for token, tag in pos_tag(word_tokenize(document)):\n",
        "            token = token.lower()\n",
        "            token = token.strip()\n",
        "            token = token.strip('_')\n",
        "            token = token.strip('*')\n",
        "\n",
        "            # Disregard stopwords\n",
        "            if token in self.stopwords:\n",
        "                continue\n",
        "\n",
        "            # Disregard punctuation\n",
        "            if all(char in self.punct for char in token):\n",
        "                continue\n",
        "\n",
        "            # yield lemmatized tokens\n",
        "            lemma = self.lemmatize(token, tag)\n",
        "            yield lemma\n",
        "\n",
        "    def lemmatize(self, token, tag):\n",
        "        tag = {\n",
        "            'N': wn.NOUN,\n",
        "            'V': wn.VERB,\n",
        "            'R': wn.ADV,\n",
        "            'J': wn.ADJ\n",
        "        }.get(tag[0], wn.NOUN)\n",
        "        return self.lemmatizer.lemmatize(token, tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGWKkATC9Wav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity(arg):\n",
        "    \"\"\"\n",
        "    Simple identity function works as a passthrough.\n",
        "    \"\"\"\n",
        "    return arg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAWjeu2n7rNU",
        "colab_type": "text"
      },
      "source": [
        "# Pipeline with preprocessor, vectorizer and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDT5rTaHV9jU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_classifier(train_x, train_y, arch, preprocessor=NLTKPreprocessor()):\n",
        "    if arch == 'svm':\n",
        "        clf = OneVsRestClassifier(estimator=LinearSVC(C=1, class_weight='balanced', dual=True,\n",
        "                                            fit_intercept=True, intercept_scaling=1,\n",
        "                                            loss='squared_hinge', max_iter=1000,\n",
        "                                            multi_class='ovr', penalty='l2',\n",
        "                                            random_state=None, tol=0.0001,\n",
        "                                            verbose=0))\n",
        "\n",
        "        word_vectorizer = TfidfVectorizer(binary=False, decode_error='strict',\n",
        "                    encoding='utf-8', dtype=np.float64,\n",
        "                    input='content', lowercase=False, max_df=0.25, max_features=None,\n",
        "                    min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
        "                    smooth_idf=True,\n",
        "                    stop_words=None,\n",
        "                    strip_accents=None, sublinear_tf=False,\n",
        "                    tokenizer=identity, use_idf=True,\n",
        "                    vocabulary=None) \n",
        "        \n",
        "\n",
        "    elif arch == 'nb':\n",
        "        clf = OneVsRestClassifier(estimator=MultinomialNB(alpha=0.01, class_prior=None,\n",
        "                                                fit_prior=True))\n",
        "\n",
        "        word_vectorizer = TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
        "                    dtype=np.float64, encoding='utf-8',\n",
        "                    input='content', lowercase=False, max_df=0.25, max_features=None,\n",
        "                    min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
        "                    smooth_idf=True,\n",
        "                    stop_words=None,\n",
        "                    strip_accents=None, sublinear_tf=False,\n",
        "                    tokenizer=identity, use_idf=True,\n",
        "                    vocabulary=None)\n",
        "        \n",
        "    \n",
        "    elif arch == 'lg':\n",
        "        clf = OneVsRestClassifier(estimator=LogisticRegression(C=1, class_weight='balanced',\n",
        "                                                     dual=False, fit_intercept=True,\n",
        "                                                     intercept_scaling=1,\n",
        "                                                     l1_ratio=None, max_iter=4000,\n",
        "                                                     multi_class='ovr',\n",
        "                                                     n_jobs=None, penalty='l2',\n",
        "                                                     random_state=None,\n",
        "                                                     solver='sag', tol=0.0001,\n",
        "                                                     verbose=0, warm_start=False),\n",
        "                                                     n_jobs=None)\n",
        "\n",
        "        word_vectorizer = TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
        "                    dtype=np.float64, encoding='utf-8',\n",
        "                    input='content', lowercase=False, max_df=0.25, max_features=None,\n",
        "                    min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
        "                    smooth_idf=True,\n",
        "                    stop_words=None,\n",
        "                    strip_accents=None, sublinear_tf=False,\n",
        "                    tokenizer=identity, use_idf=True,\n",
        "                    vocabulary=None)\n",
        "        \n",
        "    \n",
        "    elif arch == 'knn':\n",
        "        clf = KNeighborsClassifier(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
        "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
        "                     weights='distance')\n",
        "        word_vectorizer = TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
        "                dtype=np.float64, encoding='utf-8',\n",
        "                input='content', lowercase=False, max_df=0.5, max_features=None,\n",
        "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
        "                stop_words=None,\n",
        "                strip_accents=None, sublinear_tf=False,\n",
        "                tokenizer=identity, use_idf=True,\n",
        "                vocabulary=None) \n",
        "        \n",
        "\n",
        "    pipe = Pipeline([('preprocessor', preprocessor), ('tfidf', word_vectorizer), ('multilabel', clf)])\n",
        "    pipe.fit(train_x, train_y)\n",
        "    return pipe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmwDCosdpw6Q",
        "colab_type": "text"
      },
      "source": [
        "# Train the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMeghBn3zxg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(base_dir + \"dataset/cleanup_labelled.csv\")\n",
        "df.labels = df.labels.str.split('|').apply(lambda x: [int(i) for i in x])\n",
        "\n",
        "### Mask labels in text: replace all labels by SDGLABEL\n",
        "pattern = r\"(indicator)(\\s+\\d+\\.[\\d+a-d]\\.\\d+)|(target)(\\s+\\d+\\.[\\d+a-d])|(sdgs|sdg|goals|goal)\\W*\\s+(,?\\s*\\b\\d{1,2}\\b[and\\s\\b\\d{1,2}\\b]*)\"\n",
        "masked_df = df.text.str.replace(pattern, ' SDGLABEL ', regex=True, flags=re.IGNORECASE)\n",
        "# Remove double spaces\n",
        "masked_df = pd.DataFrame(masked_df.str.replace('  ', ' ', regex=True, flags=re.IGNORECASE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KlAa0AW0kz-q",
        "colab": {}
      },
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "non_masked_x = df[['text']].values \n",
        "masked_x = masked_df[['text']].values\n",
        "y = mlb.fit_transform(df.labels)\n",
        "\n",
        "labels = [str(i) for i in range(1,18)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EIY5pZzCMNLt",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "archs = ['svm', 'lg', 'knn', 'nb']\n",
        "\n",
        "for arch in archs:\n",
        "    print(f\"Processing {arch}...\")\n",
        "    for fold in os.listdir(CROSS_FOLDS):\n",
        "        print(fold)\n",
        "\n",
        "        # Load predefined indices for train, val and test\n",
        "        train_index = np.load(f\"{CROSS_FOLDS}{fold}/train.npy\")\n",
        "\n",
        "        \n",
        "        # Load train text\n",
        "        x_train = [t[0] for t in non_masked_x[train_index].tolist()]\n",
        "\n",
        "        # Load train labels\n",
        "        y_train = y[train_index]\n",
        "\n",
        "        # Fit model on fold data\n",
        "        model = run_classifier(x_train, y_train, arch=arch)\n",
        "\n",
        "        # Save model\n",
        "        save_dir = Path(f\"{OUTPUT_DIR}{arch}{mask}/\")\n",
        "        save_dir.mkdir(exist_ok=True)\n",
        "        file_dir = save_dir/f\"{arch}_{fold}.joblib\"\n",
        "        dump(model, file_dir)\n",
        "    print(f\"Finished training {arch}.\")\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dshrJceYb1O",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwmb4EOCI-Gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_avg(models_testx_testy, labels_):\n",
        "    def calc(model, test_x, test_y):\n",
        "        predictions = model.predict(test_x)\n",
        "        metrics = classification_report(test_y, predictions, target_names=labels_, output_dict=True)\n",
        "        metrics_df = pd.DataFrame.from_dict(metrics)\n",
        "        h = hamming_loss(test_y, predictions)\n",
        "        roc = roc_auc_score(test_y, predictions, average='micro')\n",
        "        return metrics_df, h, roc\n",
        "    \n",
        "    model_first, test_x_first, test_y_first = models_testx_testy[0]\n",
        "    metrics_agg, ham, roc = calc(model_first, test_x_first, test_y_first)\n",
        "    n = len(models_testx_testy)\n",
        "  \n",
        "    for model, test_x, test_y in models_testx_testy[1:]:\n",
        "        metrics, h, r = calc(model, test_x, test_y)\n",
        "        metrics_agg += metrics\n",
        "        ham += h\n",
        "        roc += r\n",
        "\n",
        "    return metrics_agg/n, ham/n, roc/n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlm6H2vmYEWD",
        "colab_type": "text"
      },
      "source": [
        "# Load and evaluate saved models on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywBE6Hdu4bQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "archs = [\"nb\", \"lg\", \"svm\", \"knn\"]\n",
        "\n",
        "#mask = \"\"\n",
        "mask = \"_masked/\"\n",
        "hr_restuls = {}\n",
        "\n",
        "for arch in archs:\n",
        "    results_dir = Path(f\"{OUTPUT_DIR}{arch}{mask}/\")\n",
        "    loaded_models = []\n",
        "    for fold in os.listdir(CROSS_FOLDS):\n",
        "        print(f\"Loading {fold} of {arch}\")\n",
        "        test_index = np.load(f\"{CROSS_FOLDS}{fold}/test.npy\")\n",
        "\n",
        "        # We used the untouched test for both masked and unmasked\n",
        "        x_test = [t[0] for t in non_masked_x[test_index].tolist()]\n",
        "\n",
        "\n",
        "        y_test = y[test_index]\n",
        "\n",
        "        load_dir = Path(f\"{OUTPUT_DIR}{arch}{mask}\")\n",
        "        load_dir = load_dir/f\"{arch}_{fold}.joblib\"\n",
        "\n",
        "        loaded_model = load(load_dir)\n",
        "        loaded_models.append((loaded_model, x_test, y_test))\n",
        "        print(f\"Finished loading the {mask} {arch} models.\")\n",
        "    \n",
        "    print(f\"Evaluating {mask} {arch} models.\")\n",
        "    loaded_results = metrics_avg(loaded_models, labels)\n",
        "    loaded_results[0].to_csv(results_dir/f'results_{arch}.csv', sep=';')\n",
        "    \n",
        "    hlos = round(loaded_results[1],4)\n",
        "    roc_auc = round(loaded_results[2],4)\n",
        "    hr_restuls[arch] = {\n",
        "        \"hl\": hlos,\n",
        "        \"roc_auc\": roc_auc\n",
        "    }\n",
        "    \n",
        "    print(f\"Finished evaluation of {mask} {arch} models.\")\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjv6SHRBR5TE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75d32f39-634d-4ebe-a9aa-e06d90ed4418"
      },
      "source": [
        "hr_restuls['knn']"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hl': 0.0739, 'roc_auc': 0.5535}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBmK1Oi8BkZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}